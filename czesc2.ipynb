{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ad6d0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes – 5-fold CV dla różnych var_smoothing:\n",
      "  var_smoothing = 1e-09 → train_acc = 0.369, test_acc = 0.359\n",
      "  var_smoothing = 1e-08 → train_acc = 0.369, test_acc = 0.359\n",
      "  var_smoothing = 1e-07 → train_acc = 0.369, test_acc = 0.359\n",
      "  var_smoothing = 1e-06 → train_acc = 0.369, test_acc = 0.359\n"
     ]
    }
   ],
   "source": [
    "# === Metoda 3: Gaussian Naive Bayes “od zera” z 5-fold CV i 4 wartościami var_smoothing ===\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def make_gnb(X_tr, y_tr, var_smooth):\n",
    "    classes = np.unique(y_tr)\n",
    "    stats = {}\n",
    "    for c in classes:\n",
    "        Xc = X_tr[y_tr == c]\n",
    "        mean = Xc.mean(axis=0)\n",
    "        var  = Xc.var(axis=0) + var_smooth\n",
    "        stats[c] = (mean, var)\n",
    "    priors = {c: np.mean(y_tr == c) for c in classes}\n",
    "    \n",
    "    def predict(Xm):\n",
    "        preds = np.empty(len(Xm), dtype=y_tr.dtype)\n",
    "        for i, x in enumerate(Xm):\n",
    "            posteriors = {}\n",
    "            for c, (mean, var) in stats.items():\n",
    "                # log prior\n",
    "                logp = math.log(priors[c])\n",
    "                # log-likelihood of Gaussian\n",
    "                log_lik = -0.5 * np.sum(np.log(2 * math.pi * var) + ((x - mean) ** 2) / var)\n",
    "                posteriors[c] = logp + log_lik\n",
    "            preds[i] = max(posteriors, key=posteriors.get)\n",
    "        return preds\n",
    "    \n",
    "    return predict\n",
    "\n",
    "def eval_gnb(var_smooth):\n",
    "    train_scores, test_scores = [], []\n",
    "    for i in range(len(folds)):\n",
    "        tr_idx = np.hstack([folds[j] for j in range(len(folds)) if j != i])\n",
    "        te_idx = folds[i]\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_te, y_te = X[te_idx], y[te_idx]\n",
    "        \n",
    "        clf = make_gnb(X_tr, y_tr, var_smooth)\n",
    "        train_scores.append((clf(X_tr) == y_tr).mean())\n",
    "        test_scores .append((clf(X_te) == y_te).mean())\n",
    "    return np.mean(train_scores), np.mean(test_scores)\n",
    "\n",
    "var_smooth_values = [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "results = {vs: eval_gnb(vs) for vs in var_smooth_values}\n",
    "\n",
    "print(\"Gaussian Naive Bayes – 5-fold CV dla różnych var_smoothing:\")\n",
    "for vs, (tr, te) in results.items():\n",
    "    print(f\"  var_smoothing = {vs:.0e} → train_acc = {tr:.3f}, test_acc = {te:.3f}\")\n",
    "\n",
    "# WNIOSKI\n",
    "# var_smoothing = 1e-9 → train_acc = 0.369, test_acc = 0.359  \n",
    "#   Minimalne wygładzenie; model jest stabilny, ale nadal osiąga niski wynik (~36%).\n",
    "# var_smoothing = 1e-8 → train_acc = 0.369, test_acc = 0.359  \n",
    "#   Dziesięciokrotnie większe wygładzenie nie zmienia accuracy – model jest już niewrażliwy na tak małe zmiany.\n",
    "# var_smoothing = 1e-7 → train_acc = 0.369, test_acc = 0.359  \n",
    "#   Kolejne zwiększenie wygładzenia również nie wpływa na wyniki – zakres parametrów za mały, by odcisnąć efekt.\n",
    "# var_smoothing = 1e-6 → train_acc = 0.369, test_acc = 0.359  \n",
    "#   Nawet przy największym testowanym wygładzeniu wyniki pozostają identyczne – var_smoothing nie ma znaczenia.\n",
    "\n",
    "# Model Gaussian Naive Bayes jest całkowicie niewrażliwy na dobór parametru var_smoothing w badanym zakresie, co \n",
    "# skazuje, że wariancje cech dominują nad drobnymi poprawkami wygładzenia. Osiągnięte ~36 % accuracy przewyższa \n",
    "# proste metody odległościowe, ale nadal pozostaje poniżej użytecznego poziomu. Aby poprawić wyniki, warto wdrożyć \n",
    "# zaawansowaną inżynierię cech, modele ensemble lub bardziej złożone klasyfikatory nieliniowe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "930a3cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB + PCA – 5-fold CV dla różnych n_components:\n",
      "  n_components =  5 → train_acc = 0.288, test_acc = 0.286\n",
      "  n_components = 10 → train_acc = 0.323, test_acc = 0.313\n",
      "  n_components = 15 → train_acc = 0.361, test_acc = 0.352\n",
      "  n_components = 17 → train_acc = 0.362, test_acc = 0.350\n"
     ]
    }
   ],
   "source": [
    "# === Metoda 3 (zaawansowana): Gaussian Naive Bayes z PCA – wpływ liczby komponentów ===\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 1) Funkcje PCA “od zera”\n",
    "def compute_pca(X, n_components):\n",
    "    mu = X.mean(axis=0)\n",
    "    Xc = X - mu\n",
    "    cov = np.cov(Xc, rowvar=False)\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    idx = np.argsort(eigvals)[::-1][:n_components]\n",
    "    comps = eigvecs[:, idx]\n",
    "    return mu, comps\n",
    "\n",
    "def transform_pca(X, mu, comps):\n",
    "    return (X - mu).dot(comps)\n",
    "\n",
    "# 2) Gaussian NB z opcjonalnym PCA\n",
    "def make_gnb_pca(X_tr, y_tr, n_comp, var_smooth=1e-9):\n",
    "    # obliczamy PCA na danych uczących\n",
    "    mu, comps = compute_pca(X_tr, n_comp)\n",
    "    Xp = transform_pca(X_tr, mu, comps)\n",
    "    # zbieramy statystyki: mean, var dla każdej klasy\n",
    "    classes = np.unique(y_tr)\n",
    "    stats = {\n",
    "        c: (\n",
    "            Xp[y_tr==c].mean(axis=0),\n",
    "            Xp[y_tr==c].var(axis=0) + var_smooth\n",
    "        )\n",
    "        for c in classes\n",
    "    }\n",
    "    priors = {c: np.mean(y_tr==c) for c in classes}\n",
    "    \n",
    "    def predict(Xm):\n",
    "        Xm_p = transform_pca(Xm, mu, comps)\n",
    "        preds = np.empty(len(Xm_p), dtype=y_tr.dtype)\n",
    "        for i, x in enumerate(Xm_p):\n",
    "            post = {}\n",
    "            for c, (m, v) in stats.items():\n",
    "                # log prior + log-likelihood Gaussa\n",
    "                logp = math.log(priors[c]) - 0.5 * np.sum(\n",
    "                    np.log(2*math.pi*v) + (x-m)**2 / v\n",
    "                )\n",
    "                post[c] = logp\n",
    "            preds[i] = max(post, key=post.get)\n",
    "        return preds\n",
    "    \n",
    "    return predict\n",
    "\n",
    "# 3) Przygotowanie par indeksów 5-fold CV\n",
    "folds_cv = [\n",
    "    (\n",
    "        np.hstack([folds[j] for j in range(len(folds)) if j != i]),\n",
    "        folds[i]\n",
    "    )\n",
    "    for i in range(len(folds))\n",
    "]\n",
    "\n",
    "# 4) Ewaluacja dla różnych liczby komponentów PCA\n",
    "components = [5, 10, 15, X.shape[1]]\n",
    "results = {}\n",
    "for n_comp in components:\n",
    "    train_accs, test_accs = [], []\n",
    "    for tr_idx, te_idx in folds_cv:\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_te, y_te = X[te_idx], y[te_idx]\n",
    "        clf = make_gnb_pca(X_tr, y_tr, n_comp)\n",
    "        train_accs.append((clf(X_tr) == y_tr).mean())\n",
    "        test_accs .append((clf(X_te) == y_te).mean())\n",
    "    results[n_comp] = (np.mean(train_accs), np.mean(test_accs))\n",
    "\n",
    "# 5) Wyświetlenie wyników\n",
    "print(\"GNB + PCA – 5-fold CV dla różnych n_components:\")\n",
    "for n_comp, (tr, te) in results.items():\n",
    "    print(f\"  n_components = {n_comp:2d} → train_acc = {tr:.3f}, test_acc = {te:.3f}\")\n",
    "\n",
    "# WNIOSKI\n",
    "# Przy bardzo redukowanej przestrzeni (5 komponentów) model traci dużo informacji, co skutkuje niskim test_acc ≈ 28.6%.\n",
    "# Zwiększenie liczby komponentów do 10 poprawia zarówno train_acc, jak i test_acc (do ≈ 31.3%).\n",
    "# Optymalny kompromis uzyskujemy przy 15 komponentach: train_acc ≈ 36.1%, test_acc ≈ 35.2%; użycie wszystkich 17 cech nie przynosi dalszej poprawy.\n",
    "# W porównaniu do poprzedniej metody bez PCA (var_smoothing), która osiągała test_acc ≈ 36%, stosowanie PCA z 15 komponentami daje niemal równorzędne wyniki,\n",
    "# ale pozwala redukować wymiarowość i potencjalnie przyspiesza obliczenia w dalszych krokach analizy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9328c0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Regression – 5-fold CV dla różnych L2 regularization:\n",
      "  reg = 0.00 → train_acc = 0.207, test_acc = 0.207\n",
      "  reg = 0.01 → train_acc = 0.200, test_acc = 0.200\n",
      "  reg = 0.10 → train_acc = 0.173, test_acc = 0.172\n",
      "  reg = 1.00 → train_acc = 0.198, test_acc = 0.198\n"
     ]
    }
   ],
   "source": [
    "# === Metoda 4: Softmax Regression (wieloklasowa regresja logistyczna) “od zera” z 5-fold CV i 4 wartościami regularyzacji L2 ===\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1) Budowa klasyfikatora Softmax Regression\n",
    "def make_logreg(X_tr, y_tr, reg, lr=0.1, epochs=200):\n",
    "    n, d = X_tr.shape\n",
    "    classes = np.unique(y_tr)\n",
    "    C = len(classes)\n",
    "    # inicjalizacja wag i biasów\n",
    "    W = np.zeros((d, C))\n",
    "    b = np.zeros(C)\n",
    "    # mapowanie etykiet na indeksy 0..C-1\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    y_idx = np.array([class_to_idx[c] for c in y_tr])\n",
    "    # one-hot\n",
    "    Y = np.eye(C)[y_idx]\n",
    "    # gradient descent\n",
    "    for _ in range(epochs):\n",
    "        scores = X_tr.dot(W) + b                          # (n, C)\n",
    "        exp_s = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
    "        P = exp_s / exp_s.sum(axis=1, keepdims=True)      # (n, C)\n",
    "        # gradient\n",
    "        dW = (X_tr.T.dot(P - Y)) / n + reg * W             # (d, C)\n",
    "        db = (P - Y).mean(axis=0)                          # (C,)\n",
    "        # update\n",
    "        W -= lr * dW\n",
    "        b -= lr * db\n",
    "    # funkcja predykcji\n",
    "    def predict(Xm):\n",
    "        out = Xm.dot(W) + b\n",
    "        return classes[np.argmax(out, axis=1)]\n",
    "    return predict\n",
    "\n",
    "# 2) Funkcja ewaluacji dla danego reg w 5-fold CV\n",
    "def eval_reg(reg):\n",
    "    train_scores, test_scores = [], []\n",
    "    for i in range(len(folds)):\n",
    "        tr_idx = np.hstack([folds[j] for j in range(len(folds)) if j != i])\n",
    "        te_idx = folds[i]\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_te, y_te = X[te_idx], y[te_idx]\n",
    "        clf = make_logreg(X_tr, y_tr, reg)\n",
    "        train_scores.append((clf(X_tr) == y_tr).mean())\n",
    "        test_scores .append((clf(X_te) == y_te).mean())\n",
    "    return np.mean(train_scores), np.mean(test_scores)\n",
    "\n",
    "# 3) Testujemy 4 wartości regularyzacji L2\n",
    "regs = [0.0, 0.01, 0.1, 1.0]\n",
    "results = {r: eval_reg(r) for r in regs}\n",
    "\n",
    "# 4) Wyświetlenie wyników\n",
    "print(\"Softmax Regression – 5-fold CV dla różnych L2 regularization:\")\n",
    "for r, (tr, te) in results.items():\n",
    "    print(f\"  reg = {r:.2f} → train_acc = {tr:.3f}, test_acc = {te:.3f}\")\n",
    "\n",
    "# WNIOSKI\n",
    "# reg = 0.00 → train_acc = 0.207, test_acc = 0.207  \n",
    "#   Brak regularyzacji daje najlepsze wyniki, model nie wykazuje nadmiernego overfittingu.\n",
    "# reg = 0.01 → train_acc = 0.200, test_acc = 0.200  \n",
    "#   Lekka regularyzacja obniża accuracy o ~0.7 p.p., sugerując niewielki wpływ.\n",
    "# reg = 0.10 → train_acc = 0.173, test_acc = 0.172  \n",
    "#   Silniejsza regularyzacja prowadzi do underfittingu i znacznego spadku dokładności.\n",
    "# reg = 1.00 → train_acc = 0.198, test_acc = 0.198  \n",
    "#   Bardzo duże L2 częściowo przywraca dopasowanie, lecz nadal nie dorównuje modelowi bez regularyzacji.\n",
    "\n",
    "# Softmax regression osiąga jedynie około 20 % accuracy niezależnie od poziomu L2, co świadczy o tym, że \n",
    "# liniowe granice decyzyjne są zbyt proste dla tych danych. Brak regularyzacji daje najlepsze rezultaty, co \n",
    "# oznacza, że model nie overfituje znacząco, ale też nie potrafi efektywnie uogólniać. Aby uzyskać sensowną \n",
    "# poprawę wyników, warto sięgnąć po bardziej złożone klasyfikatory lub zaawansowaną inżynierię cech.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d4b793a-bb8a-412d-8fae-a63a36be34bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testowanie 10 drzew...\n",
      "Testowanie 50 drzew...\n",
      "Testowanie 100 drzew...\n",
      "Testowanie 200 drzew...\n",
      "\n",
      "Random Forest – 5-fold CV (implementacja ręczna):\n",
      "  n_estimators =  10 → train_acc = 0.538, test_acc = 0.375\n",
      "  n_estimators =  50 → train_acc = 0.563, test_acc = 0.394\n",
      "  n_estimators = 100 → train_acc = 0.569, test_acc = 0.391\n",
      "  n_estimators = 200 → train_acc = 0.574, test_acc = 0.392\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# === 1) Podstawowe drzewo decyzyjne do klasyfikacji ===\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=10, min_samples_split=2, n_feats=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_feats = n_feats\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(set(y))\n",
    "        self.n_features = X.shape[1] if self.n_feats is None else self.n_feats\n",
    "        self.tree = self._grow(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(x, self.tree) for x in X])\n",
    "\n",
    "    def _gini(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        probs = counts / len(y)\n",
    "        return 1 - np.sum(probs ** 2)\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gini = 1\n",
    "        split_idx, split_thresh = None, None\n",
    "        for feat in feat_idxs:\n",
    "            thresholds = np.unique(X[:, feat])\n",
    "            for thresh in thresholds:\n",
    "                left = y[X[:, feat] <= thresh]\n",
    "                right = y[X[:, feat] > thresh]\n",
    "                if len(left) == 0 or len(right) == 0:\n",
    "                    continue\n",
    "                g = (len(left) * self._gini(left) + len(right) * self._gini(right)) / len(y)\n",
    "                if g < best_gini:\n",
    "                    best_gini = g\n",
    "                    split_idx = feat\n",
    "                    split_thresh = thresh\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _grow(self, X, y, depth):\n",
    "        if (depth >= self.max_depth or len(y) < self.min_samples_split or len(set(y)) == 1):\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "        feat_idxs = np.random.choice(X.shape[1], self.n_features, replace=False)\n",
    "        idx, thresh = self._best_split(X, y, feat_idxs)\n",
    "        if idx is None:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "        left_idx = X[:, idx] <= thresh\n",
    "        right_idx = X[:, idx] > thresh\n",
    "        left = self._grow(X[left_idx], y[left_idx], depth + 1)\n",
    "        right = self._grow(X[right_idx], y[right_idx], depth + 1)\n",
    "        return (idx, thresh, left, right)\n",
    "\n",
    "    def _predict(self, x, node):\n",
    "        if not isinstance(node, tuple):\n",
    "            return node\n",
    "        idx, thresh, left, right = node\n",
    "        if x[idx] <= thresh:\n",
    "            return self._predict(x, left)\n",
    "        else:\n",
    "            return self._predict(x, right)\n",
    "\n",
    "# === 2) Random Forest od zera ===\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=100, max_depth=10, min_samples_split=2):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_feats = int(np.sqrt(X.shape[1]))\n",
    "        for _ in range(self.n_trees):\n",
    "            idxs = np.random.choice(len(X), len(X), replace=True)\n",
    "            tree = DecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                n_feats=n_feats\n",
    "            )\n",
    "            tree.fit(X[idxs], y[idxs])\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.array([\n",
    "            Counter(tree_preds[:, i]).most_common(1)[0][0]\n",
    "            for i in range(X.shape[0])\n",
    "        ])\n",
    "\n",
    "# === 3) 5-fold cross-validation i test n_estimators ===\n",
    "def stratified_folds(y, k=5):\n",
    "    np.random.seed(0)\n",
    "    folds = [[] for _ in range(k)]\n",
    "    for cls in np.unique(y):\n",
    "        idx = np.where(y == cls)[0]\n",
    "        np.random.shuffle(idx)\n",
    "        for i, ix in enumerate(idx):\n",
    "            folds[i % k].append(ix)\n",
    "    return [np.array(f) for f in folds]\n",
    "\n",
    "def crossval_rf(X, y, n_trees):\n",
    "    folds = stratified_folds(y)\n",
    "    train_accs, test_accs = [], []\n",
    "    for i in range(5):\n",
    "        test_idx = folds[i]\n",
    "        train_idx = np.hstack([folds[j] for j in range(5) if j != i])\n",
    "        X_tr, y_tr = X[train_idx], y[train_idx]\n",
    "        X_te, y_te = X[test_idx], y[test_idx]\n",
    "        rf = RandomForest(n_trees=n_trees, max_depth=10)\n",
    "        rf.fit(X_tr, y_tr)\n",
    "        train_preds = rf.predict(X_tr)\n",
    "        test_preds = rf.predict(X_te)\n",
    "        train_accs.append((train_preds == y_tr).mean())\n",
    "        test_accs.append((test_preds == y_te).mean())\n",
    "    return np.mean(train_accs), np.mean(test_accs)\n",
    "\n",
    "# === 4) Wczytaj dane i uruchom testy ===\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\48888\\PycharmProjects\\sieci_neuronowe\\train_data.csv', sep=';').dropna()\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col], _ = pd.factorize(df[col])\n",
    "np.random.seed(0)\n",
    "df = df.sample(10000).reset_index(drop=True)\n",
    "X = df.drop(columns='Stay').values\n",
    "y = df['Stay'].values\n",
    "\n",
    "n_trees_list = [10, 50, 100, 200]\n",
    "results = {}\n",
    "for n in n_trees_list:\n",
    "    print(f'Testowanie {n} drzew...')\n",
    "    train_acc, test_acc = crossval_rf(X, y, n)\n",
    "    results[n] = (train_acc, test_acc)\n",
    "\n",
    "print(\"\\nRandom Forest – 5-fold CV (implementacja ręczna):\")\n",
    "for n, (tr, te) in results.items():\n",
    "    print(f\"  n_estimators = {n:3d} → train_acc = {tr:.3f}, test_acc = {te:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a733a2c2-81df-4556-96ee-34266f2f82ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9a32e2b-f593-4157-81de-3879cfd9c673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przetwarzanie k=1 (25.0% wszystkich)\n",
      "[k=1] Fold 5/5 (100.0%)\n",
      "Przetwarzanie k=3 (50.0% wszystkich)\n",
      "[k=3] Fold 5/5 (100.0%)\n",
      "Przetwarzanie k=5 (75.0% wszystkich)\n",
      "[k=5] Fold 5/5 (100.0%)\n",
      "Przetwarzanie k=7 (100.0% wszystkich)\n",
      "[k=7] Fold 5/5 (100.0%)\n",
      "\n",
      "k-NN (po standaryzacji) – 5-fold CV:\n",
      "  k =  1 → train_acc = 1.000, test_acc = 0.256\n",
      "  k =  3 → train_acc = 0.835, test_acc = 0.278\n",
      "  k =  5 → train_acc = 0.645, test_acc = 0.299\n",
      "  k =  7 → train_acc = 0.584, test_acc = 0.309\n"
     ]
    }
   ],
   "source": [
    "#Metoda 1 ale ze standaryzacją\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# === 1) Wczytanie i przygotowanie danych ===\n",
    "df = pd.read_csv(r'C:\\Users\\48888\\PycharmProjects\\sieci_neuronowe\\train_data.csv', sep=';').dropna()\n",
    "\n",
    "# Kodowanie zmiennych kategorycznych (stringów) na liczby\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col], _ = pd.factorize(df[col])\n",
    "\n",
    "# Subsampling – wybieramy 10 000 losowych próbek\n",
    "np.random.seed(0)\n",
    "df = df.sample(10000).reset_index(drop=True)\n",
    "\n",
    "X_full = df.drop('Stay', axis=1).values\n",
    "y_full = df['Stay'].values\n",
    "\n",
    "# === 2) Stratified 5-fold Cross-Validation ===\n",
    "def make_folds(y, k=5, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    folds = [[] for _ in range(k)]\n",
    "    for cls in np.unique(y):\n",
    "        cls_idx = np.where(y == cls)[0]\n",
    "        np.random.shuffle(cls_idx)\n",
    "        for i, ix in enumerate(cls_idx):\n",
    "            folds[i % k].append(ix)\n",
    "    return [np.array(f) for f in folds]\n",
    "\n",
    "folds = make_folds(y_full, k=5)\n",
    "\n",
    "# === 3) Implementacja k-NN “od zera” ===\n",
    "def make_knn(X_tr, y_tr, k, chunk_size=200):\n",
    "    def predict(Xm):\n",
    "        preds = np.empty(len(Xm), dtype=y_tr.dtype)\n",
    "        for start in range(0, len(Xm), chunk_size):\n",
    "            end = min(start + chunk_size, len(Xm))\n",
    "            batch = Xm[start:end]\n",
    "            D = np.linalg.norm(batch[:, None, :] - X_tr[None, :, :], axis=2)\n",
    "            nn = np.argpartition(D, k, axis=1)[:, :k]\n",
    "            for i, neigh in enumerate(nn):\n",
    "                preds[start + i] = Counter(y_tr[neigh]).most_common(1)[0][0]\n",
    "        return preds\n",
    "    return predict\n",
    "\n",
    "# === 4) Standaryzacja danych (ręczna, zrobiona osobno dla każdego folda) ===\n",
    "def standardize(X_train, X_test):\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0)\n",
    "    std[std == 0] = 1  # zapobiega dzieleniu przez zero\n",
    "    X_train_std = (X_train - mean) / std\n",
    "    X_test_std = (X_test - mean) / std\n",
    "    return X_train_std, X_test_std\n",
    "\n",
    "# === 5) Ewaluacja k-NN z ręczną standaryzacją ===\n",
    "def eval_k(k):\n",
    "    train_scores, test_scores = [], []\n",
    "    total = len(folds)\n",
    "    for i in range(total):\n",
    "        print(f'\\r[k={k}] Fold {i+1}/{total} ({(i+1)/total*100:.1f}%)', end='', flush=True)\n",
    "        te = folds[i]\n",
    "        tr = np.hstack([folds[j] for j in range(total) if j != i])\n",
    "        X_tr_raw, y_tr = X_full[tr], y_full[tr]\n",
    "        X_te_raw, y_te = X_full[te], y_full[te]\n",
    "\n",
    "        # === Ręczna standaryzacja\n",
    "        X_tr, X_te = standardize(X_tr_raw, X_te_raw)\n",
    "\n",
    "        clf = make_knn(X_tr, y_tr, k)\n",
    "        train_scores.append((clf(X_tr) == y_tr).mean())\n",
    "        test_scores.append((clf(X_te) == y_te).mean())\n",
    "    print()\n",
    "    return np.mean(train_scores), np.mean(test_scores)\n",
    "\n",
    "# === 6) Testowanie różnych wartości k ===\n",
    "k_values = [1, 3, 5, 7]\n",
    "results = {}\n",
    "\n",
    "for i, k in enumerate(k_values):\n",
    "    print(f'Przetwarzanie k={k} ({(i+1)/len(k_values)*100:.1f}% wszystkich)')\n",
    "    results[k] = eval_k(k)\n",
    "\n",
    "# === 7) Wyświetlenie wyników ===\n",
    "print(\"\\nk-NN (po standaryzacji) – 5-fold CV:\")\n",
    "for k, (tr, te) in results.items():\n",
    "    print(f\"  k = {k:2d} → train_acc = {tr:.3f}, test_acc = {te:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e98772c0-8dd2-4d08-b928-5d318802676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Centroid (ze standaryzacją) – 5-fold CV:\n",
      "  euclidean  → train_acc = 0.232, test_acc = 0.221\n",
      "  manhattan  → train_acc = 0.217, test_acc = 0.203\n",
      "  chebyshev  → train_acc = 0.138, test_acc = 0.135\n",
      "  cosine     → train_acc = 0.234, test_acc = 0.221\n"
     ]
    }
   ],
   "source": [
    "#Metoda 2 ale ze standaryzacją\n",
    "import numpy as np\n",
    "\n",
    "# === Funkcja do ręcznej standaryzacji danych ===\n",
    "def standardize(X_train, X_test):\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0)\n",
    "    std[std == 0] = 1  # unikamy dzielenia przez zero\n",
    "    X_train_std = (X_train - mean) / std\n",
    "    X_test_std = (X_test - mean) / std\n",
    "    return X_train_std, X_test_std\n",
    "\n",
    "# === Funkcja klasyfikatora Nearest Centroid z wybraną metryką ===\n",
    "def make_centroid(X_tr, y_tr, metric):\n",
    "    classes = np.unique(y_tr)\n",
    "    cents = {c: X_tr[y_tr == c].mean(axis=0) for c in classes}\n",
    "\n",
    "    def dist(a, b):\n",
    "        if metric == 'euclidean':\n",
    "            return np.linalg.norm(a - b)\n",
    "        elif metric == 'manhattan':\n",
    "            return np.sum(np.abs(a - b))\n",
    "        elif metric == 'chebyshev':\n",
    "            return np.max(np.abs(a - b))\n",
    "        else:  # cosine\n",
    "            return 1 - (a @ b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "    def predict(Xm):\n",
    "        preds = np.empty(len(Xm), dtype=y_tr.dtype)\n",
    "        for i, x in enumerate(Xm):\n",
    "            dists = [dist(x, cents[c]) for c in classes]\n",
    "            preds[i] = classes[np.argmin(dists)]\n",
    "        return preds\n",
    "\n",
    "    return predict\n",
    "\n",
    "# === Funkcja ewaluacji metryki z 5-fold CV i standaryzacją ===\n",
    "def eval_metric_std(metric):\n",
    "    train_scores, test_scores = [], []\n",
    "    for i in range(len(folds)):\n",
    "        tr_idx = np.hstack([folds[j] for j in range(len(folds)) if j != i])\n",
    "        te_idx = folds[i]\n",
    "\n",
    "        X_tr_raw, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_te_raw, y_te = X[te_idx], y[te_idx]\n",
    "\n",
    "        # === Ręczna standaryzacja\n",
    "        X_tr, X_te = standardize(X_tr_raw, X_te_raw)\n",
    "\n",
    "        clf = make_centroid(X_tr, y_tr, metric)\n",
    "        train_scores.append((clf(X_tr) == y_tr).mean())\n",
    "        test_scores.append((clf(X_te) == y_te).mean())\n",
    "    return np.mean(train_scores), np.mean(test_scores)\n",
    "\n",
    "# === Testujemy 4 metryki odległości ===\n",
    "metrics = ['euclidean', 'manhattan', 'chebyshev', 'cosine']\n",
    "results = {m: eval_metric_std(m) for m in metrics}\n",
    "\n",
    "# === Wyświetlenie wyników ===\n",
    "print(\"Nearest Centroid (ze standaryzacją) – 5-fold CV:\")\n",
    "for m, (tr, te) in results.items():\n",
    "    print(f\"  {m:<10} → train_acc = {tr:.3f}, test_acc = {te:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf989848-6dbe-49d5-a5f2-33326c79136f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
