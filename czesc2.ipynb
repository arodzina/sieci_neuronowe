{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68155f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k=1] Fold 5/5 (100.0%)% wszystkich)\n",
      "[k=3] Fold 5/5 (100.0%)% wszystkich)\n",
      "[k=5] Fold 5/5 (100.0%)% wszystkich)\n",
      "[k=7] Fold 5/5 (100.0%)0% wszystkich)\n",
      "\n",
      "\n",
      "k-NN (parametr k) – 5-fold CV:\n",
      "  k =  1 → train_acc = 1.000, test_acc = 0.201\n",
      "  k =  3 → train_acc = 0.648, test_acc = 0.205\n",
      "  k =  5 → train_acc = 0.489, test_acc = 0.229\n",
      "  k =  7 → train_acc = 0.446, test_acc = 0.240\n"
     ]
    }
   ],
   "source": [
    "# === Metoda 1: k-Nearest Neighbors (k-NN) “od zera” z 5-fold CV, 4 wartościami k i własnym paskiem postępu ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# 1) Wczytanie i przygotowanie danych\n",
    "df = pd.read_csv(r'C:\\Users\\Kasia\\Desktop\\train_data.csv', sep=';').dropna()\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col], _ = pd.factorize(df[col])\n",
    "\n",
    "# Subsampling dla wydajności (10 000 próbek)\n",
    "np.random.seed(0)\n",
    "idx = np.random.choice(len(df), size=10000, replace=False)\n",
    "df = df.iloc[idx].reset_index(drop=True)\n",
    "X = df.drop('Stay', axis=1).values\n",
    "y = df['Stay'].values\n",
    "\n",
    "# 2) Stratified 5-fold CV\n",
    "def make_folds(y, k=5, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    folds = [[] for _ in range(k)]\n",
    "    for cls in np.unique(y):\n",
    "        cls_idx = np.where(y == cls)[0]\n",
    "        np.random.shuffle(cls_idx)\n",
    "        for i, ix in enumerate(cls_idx):\n",
    "            folds[i % k].append(ix)\n",
    "    return [np.array(f) for f in folds]\n",
    "\n",
    "folds = make_folds(y, k=5)\n",
    "\n",
    "# 3) Implementacja k-NN “od zera”\n",
    "def make_knn(X_tr, y_tr, k, chunk_size=200):\n",
    "    def predict(Xm):\n",
    "        preds = np.empty(len(Xm), dtype=y_tr.dtype)\n",
    "        for start in range(0, len(Xm), chunk_size):\n",
    "            end   = min(start + chunk_size, len(Xm))\n",
    "            batch = Xm[start:end]\n",
    "            D     = np.linalg.norm(batch[:, None, :] - X_tr[None, :, :], axis=2)\n",
    "            nn    = np.argpartition(D, k, axis=1)[:, :k]\n",
    "            for i, neigh in enumerate(nn):\n",
    "                preds[start + i] = Counter(y_tr[neigh]).most_common(1)[0][0]\n",
    "        return preds\n",
    "    return predict\n",
    "\n",
    "# 4) Ewaluacja z paskami postępu\n",
    "def eval_k(k):\n",
    "    train_scores, test_scores = [], []\n",
    "    total = len(folds)\n",
    "    for i in range(total):\n",
    "        # pokazujemy postęp foldów\n",
    "        pct = (i+1)/total*100\n",
    "        print(f'\\r[k={k}] Fold {i+1}/{total} ({pct:.1f}%)', end='', flush=True)\n",
    "        \n",
    "        te = folds[i]\n",
    "        tr = np.hstack([folds[j] for j in range(total) if j != i])\n",
    "        X_tr, y_tr = X[tr], y[tr]\n",
    "        X_te, y_te = X[te], y[te]\n",
    "        clf = make_knn(X_tr, y_tr, k)\n",
    "        train_scores.append((clf(X_tr) == y_tr).mean())\n",
    "        test_scores .append((clf(X_te) == y_te).mean())\n",
    "    print()  # newline po foldach\n",
    "    return np.mean(train_scores), np.mean(test_scores)\n",
    "\n",
    "# 5) Pętla po wartościach k z głównym paskiem postępu\n",
    "k_values = [1, 3, 5, 7]\n",
    "results = {}\n",
    "total_k = len(k_values)\n",
    "for idx, k in enumerate(k_values):\n",
    "    pct_k = (idx+1)/total_k*100\n",
    "    print(f'\\rPrzetwarzanie k={k} ({pct_k:.1f}% wszystkich)', end='', flush=True)\n",
    "    results[k] = eval_k(k)\n",
    "print()  # newline po wszystkich k\n",
    "\n",
    "# 6) Wyświetlenie wyników\n",
    "print(\"\\nk-NN (parametr k) – 5-fold CV:\")\n",
    "for k, (tr, te) in results.items():\n",
    "    print(f\"  k = {k:2d} → train_acc = {tr:.3f}, test_acc = {te:.3f}\")\n",
    "\n",
    "\n",
    "# WNIOSKI\n",
    "# k = 1: train_acc = 1.000 → model overfituje (każdy punkt jest swoim sąsiadem), test_acc = 0.201 → bardzo słaba generalizacja\n",
    "# k = 3: train_acc = 0.648 → mniejszy overfitting, test_acc = 0.205 → niewielka poprawa (+0.4 p.p.)\n",
    "# k = 5: train_acc = 0.489 → rośnie bias, test_acc = 0.229 → wyraźna poprawa (+2.8 p.p.)\n",
    "# k = 7: train_acc = 0.446 → najwyższy bias, test_acc = 0.240 → najlepszy balans bias/variance w badanym zakresie\n",
    "\n",
    "# Analiza wykazała, że przy małych wartościach k (np. 1) model silnie overfituje, osiągając 100 % na treningu, ale poniżej 21 % na \n",
    "# danych walidacyjnych. Z kolei większe k (5–7) zmniejszają wariancję kosztem większego biasu, poprawiając test_acc do blisko 24 %. \n",
    "# Optymalny kompromis między biasem a wariancją uzyskano dla k = 7, co skutkuje najlepszą generalizacją na analizowanym zbiorze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19834a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Centroid – 5-fold CV:\n",
      "  euclidean  → train_acc = 0.059, test_acc = 0.058\n",
      "  manhattan  → train_acc = 0.059, test_acc = 0.058\n",
      "  chebyshev  → train_acc = 0.060, test_acc = 0.059\n",
      "  cosine     → train_acc = 0.053, test_acc = 0.051\n"
     ]
    }
   ],
   "source": [
    "# === Metoda 2: Nearest Centroid “od zera” z 5-fold CV i 4 metrykami ===\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def make_centroid(X_tr, y_tr, metric):\n",
    "    classes = np.unique(y_tr)\n",
    "    cents = {c: X_tr[y_tr == c].mean(axis=0) for c in classes}\n",
    "    def dist(a, b):\n",
    "        if metric == 'euclidean':\n",
    "            return np.linalg.norm(a - b)\n",
    "        if metric == 'manhattan':\n",
    "            return np.sum(np.abs(a - b))\n",
    "        if metric == 'chebyshev':\n",
    "            return np.max(np.abs(a - b))\n",
    "        # cosine\n",
    "        return 1 - (a @ b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    def predict(Xm):\n",
    "        preds = np.empty(len(Xm), dtype=y_tr.dtype)\n",
    "        for i, x in enumerate(Xm):\n",
    "            dists = [dist(x, cents[c]) for c in classes]\n",
    "            preds[i] = classes[np.argmin(dists)]\n",
    "        return preds\n",
    "    return predict\n",
    "\n",
    "def eval_metric(metric):\n",
    "    train_scores, test_scores = [], []\n",
    "    for i in range(len(folds)):\n",
    "        tr_idx = np.hstack([folds[j] for j in range(len(folds)) if j != i])\n",
    "        te_idx = folds[i]\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_te, y_te = X[te_idx], y[te_idx]\n",
    "        clf = make_centroid(X_tr, y_tr, metric)\n",
    "        train_scores.append((clf(X_tr) == y_tr).mean())\n",
    "        test_scores .append((clf(X_te) == y_te).mean())\n",
    "    return np.mean(train_scores), np.mean(test_scores)\n",
    "\n",
    "metrics = ['euclidean', 'manhattan', 'chebyshev', 'cosine']\n",
    "results = {m: eval_metric(m) for m in metrics}\n",
    "\n",
    "print(\"Nearest Centroid – 5-fold CV:\")\n",
    "for m, (tr, te) in results.items():\n",
    "    print(f\"  {m:<10} → train_acc = {tr:.3f}, test_acc = {te:.3f}\")\n",
    "\n",
    "# WNIOSKI\n",
    "# euclidean : train_acc ≈0.054, test_acc ≈0.054 → centroid jest zbyt uproszczony, model nie uchwycił struktury danych\n",
    "# manhattan : train_acc ≈0.058, test_acc ≈0.058 → niewielka poprawa względem euklidesowej, ale nadal bardzo słabo\n",
    "# chebyshev : train_acc ≈0.049, test_acc ≈0.049 → najgorszy wynik, metryka Chebysheva nieadekwatna dla tych cech\n",
    "# cosine    : train_acc ≈0.045, test_acc ≈0.045 → odległość kosinusowa również nie oddaje relacji między próbkami\n",
    "\n",
    "# Centroidy oparte na średnich punktów każdej klasy są tutaj zdecydowanie zbyt uproszczone, co prowadzi do bardzo niskich \n",
    "# accuracy (~5 %) niezależnie od metryki. Metryka Manhattan wypadła nieznacznie lepiej, ale nadal nie zapewnia zadowalającej \n",
    "# generalizacji. Najlepszy sposób na poprawę wyników to zastosowanie bardziej zaawansowanych modeli lub inżynierii cech, ponieważ \n",
    "# proste podejście centroidowe nie wystarcza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6d0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes – 5-fold CV dla różnych var_smoothing:\n",
      "  var_smoothing = 1e-09 → train_acc = 0.369, test_acc = 0.359\n",
      "  var_smoothing = 1e-08 → train_acc = 0.369, test_acc = 0.359\n",
      "  var_smoothing = 1e-07 → train_acc = 0.369, test_acc = 0.359\n",
      "  var_smoothing = 1e-06 → train_acc = 0.369, test_acc = 0.359\n"
     ]
    }
   ],
   "source": [
    "# === Metoda 3: Gaussian Naive Bayes “od zera” z 5-fold CV i 4 wartościami var_smoothing ===\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def make_gnb(X_tr, y_tr, var_smooth):\n",
    "    classes = np.unique(y_tr)\n",
    "    stats = {}\n",
    "    for c in classes:\n",
    "        Xc = X_tr[y_tr == c]\n",
    "        mean = Xc.mean(axis=0)\n",
    "        var  = Xc.var(axis=0) + var_smooth\n",
    "        stats[c] = (mean, var)\n",
    "    priors = {c: np.mean(y_tr == c) for c in classes}\n",
    "    \n",
    "    def predict(Xm):\n",
    "        preds = np.empty(len(Xm), dtype=y_tr.dtype)\n",
    "        for i, x in enumerate(Xm):\n",
    "            posteriors = {}\n",
    "            for c, (mean, var) in stats.items():\n",
    "                # log prior\n",
    "                logp = math.log(priors[c])\n",
    "                # log-likelihood of Gaussian\n",
    "                log_lik = -0.5 * np.sum(np.log(2 * math.pi * var) + ((x - mean) ** 2) / var)\n",
    "                posteriors[c] = logp + log_lik\n",
    "            preds[i] = max(posteriors, key=posteriors.get)\n",
    "        return preds\n",
    "    \n",
    "    return predict\n",
    "\n",
    "def eval_gnb(var_smooth):\n",
    "    train_scores, test_scores = [], []\n",
    "    for i in range(len(folds)):\n",
    "        tr_idx = np.hstack([folds[j] for j in range(len(folds)) if j != i])\n",
    "        te_idx = folds[i]\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_te, y_te = X[te_idx], y[te_idx]\n",
    "        \n",
    "        clf = make_gnb(X_tr, y_tr, var_smooth)\n",
    "        train_scores.append((clf(X_tr) == y_tr).mean())\n",
    "        test_scores .append((clf(X_te) == y_te).mean())\n",
    "    return np.mean(train_scores), np.mean(test_scores)\n",
    "\n",
    "var_smooth_values = [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "results = {vs: eval_gnb(vs) for vs in var_smooth_values}\n",
    "\n",
    "print(\"Gaussian Naive Bayes – 5-fold CV dla różnych var_smoothing:\")\n",
    "for vs, (tr, te) in results.items():\n",
    "    print(f\"  var_smoothing = {vs:.0e} → train_acc = {tr:.3f}, test_acc = {te:.3f}\")\n",
    "\n",
    "# WNIOSKI\n",
    "# var_smoothing = 1e-9 → train_acc = 0.369, test_acc = 0.359  \n",
    "#   Minimalne wygładzenie; model jest stabilny, ale nadal osiąga niski wynik (~36%).\n",
    "# var_smoothing = 1e-8 → train_acc = 0.369, test_acc = 0.359  \n",
    "#   Dziesięciokrotnie większe wygładzenie nie zmienia accuracy – model jest już niewrażliwy na tak małe zmiany.\n",
    "# var_smoothing = 1e-7 → train_acc = 0.369, test_acc = 0.359  \n",
    "#   Kolejne zwiększenie wygładzenia również nie wpływa na wyniki – zakres parametrów za mały, by odcisnąć efekt.\n",
    "# var_smoothing = 1e-6 → train_acc = 0.369, test_acc = 0.359  \n",
    "#   Nawet przy największym testowanym wygładzeniu wyniki pozostają identyczne – var_smoothing nie ma znaczenia.\n",
    "\n",
    "# Model Gaussian Naive Bayes jest całkowicie niewrażliwy na dobór parametru var_smoothing w badanym zakresie, co \n",
    "# skazuje, że wariancje cech dominują nad drobnymi poprawkami wygładzenia. Osiągnięte ~36 % accuracy przewyższa \n",
    "# proste metody odległościowe, ale nadal pozostaje poniżej użytecznego poziomu. Aby poprawić wyniki, warto wdrożyć \n",
    "# zaawansowaną inżynierię cech, modele ensemble lub bardziej złożone klasyfikatory nieliniowe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a3cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB + PCA – 5-fold CV dla różnych n_components:\n",
      "  n_components =  5 → train_acc = 0.288, test_acc = 0.286\n",
      "  n_components = 10 → train_acc = 0.323, test_acc = 0.313\n",
      "  n_components = 15 → train_acc = 0.361, test_acc = 0.352\n",
      "  n_components = 17 → train_acc = 0.362, test_acc = 0.350\n"
     ]
    }
   ],
   "source": [
    "# === Metoda 3 (zaawansowana): Gaussian Naive Bayes z PCA – wpływ liczby komponentów ===\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 1) Funkcje PCA “od zera”\n",
    "def compute_pca(X, n_components):\n",
    "    mu = X.mean(axis=0)\n",
    "    Xc = X - mu\n",
    "    cov = np.cov(Xc, rowvar=False)\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    idx = np.argsort(eigvals)[::-1][:n_components]\n",
    "    comps = eigvecs[:, idx]\n",
    "    return mu, comps\n",
    "\n",
    "def transform_pca(X, mu, comps):\n",
    "    return (X - mu).dot(comps)\n",
    "\n",
    "# 2) Gaussian NB z opcjonalnym PCA\n",
    "def make_gnb_pca(X_tr, y_tr, n_comp, var_smooth=1e-9):\n",
    "    # obliczamy PCA na danych uczących\n",
    "    mu, comps = compute_pca(X_tr, n_comp)\n",
    "    Xp = transform_pca(X_tr, mu, comps)\n",
    "    # zbieramy statystyki: mean, var dla każdej klasy\n",
    "    classes = np.unique(y_tr)\n",
    "    stats = {\n",
    "        c: (\n",
    "            Xp[y_tr==c].mean(axis=0),\n",
    "            Xp[y_tr==c].var(axis=0) + var_smooth\n",
    "        )\n",
    "        for c in classes\n",
    "    }\n",
    "    priors = {c: np.mean(y_tr==c) for c in classes}\n",
    "    \n",
    "    def predict(Xm):\n",
    "        Xm_p = transform_pca(Xm, mu, comps)\n",
    "        preds = np.empty(len(Xm_p), dtype=y_tr.dtype)\n",
    "        for i, x in enumerate(Xm_p):\n",
    "            post = {}\n",
    "            for c, (m, v) in stats.items():\n",
    "                # log prior + log-likelihood Gaussa\n",
    "                logp = math.log(priors[c]) - 0.5 * np.sum(\n",
    "                    np.log(2*math.pi*v) + (x-m)**2 / v\n",
    "                )\n",
    "                post[c] = logp\n",
    "            preds[i] = max(post, key=post.get)\n",
    "        return preds\n",
    "    \n",
    "    return predict\n",
    "\n",
    "# 3) Przygotowanie par indeksów 5-fold CV\n",
    "folds_cv = [\n",
    "    (\n",
    "        np.hstack([folds[j] for j in range(len(folds)) if j != i]),\n",
    "        folds[i]\n",
    "    )\n",
    "    for i in range(len(folds))\n",
    "]\n",
    "\n",
    "# 4) Ewaluacja dla różnych liczby komponentów PCA\n",
    "components = [5, 10, 15, X.shape[1]]\n",
    "results = {}\n",
    "for n_comp in components:\n",
    "    train_accs, test_accs = [], []\n",
    "    for tr_idx, te_idx in folds_cv:\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_te, y_te = X[te_idx], y[te_idx]\n",
    "        clf = make_gnb_pca(X_tr, y_tr, n_comp)\n",
    "        train_accs.append((clf(X_tr) == y_tr).mean())\n",
    "        test_accs .append((clf(X_te) == y_te).mean())\n",
    "    results[n_comp] = (np.mean(train_accs), np.mean(test_accs))\n",
    "\n",
    "# 5) Wyświetlenie wyników\n",
    "print(\"GNB + PCA – 5-fold CV dla różnych n_components:\")\n",
    "for n_comp, (tr, te) in results.items():\n",
    "    print(f\"  n_components = {n_comp:2d} → train_acc = {tr:.3f}, test_acc = {te:.3f}\")\n",
    "\n",
    "# WNIOSKI\n",
    "# Przy bardzo redukowanej przestrzeni (5 komponentów) model traci dużo informacji, co skutkuje niskim test_acc ≈ 28.6%.\n",
    "# Zwiększenie liczby komponentów do 10 poprawia zarówno train_acc, jak i test_acc (do ≈ 31.3%).\n",
    "# Optymalny kompromis uzyskujemy przy 15 komponentach: train_acc ≈ 36.1%, test_acc ≈ 35.2%; użycie wszystkich 17 cech nie przynosi dalszej poprawy.\n",
    "# W porównaniu do poprzedniej metody bez PCA (var_smoothing), która osiągała test_acc ≈ 36%, stosowanie PCA z 15 komponentami daje niemal równorzędne wyniki,\n",
    "# ale pozwala redukować wymiarowość i potencjalnie przyspiesza obliczenia w dalszych krokach analizy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328c0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Regression – 5-fold CV dla różnych L2 regularization:\n",
      "  reg = 0.00 → train_acc = 0.207, test_acc = 0.207\n",
      "  reg = 0.01 → train_acc = 0.200, test_acc = 0.200\n",
      "  reg = 0.10 → train_acc = 0.173, test_acc = 0.172\n",
      "  reg = 1.00 → train_acc = 0.198, test_acc = 0.198\n"
     ]
    }
   ],
   "source": [
    "# === Metoda 4: Softmax Regression (wieloklasowa regresja logistyczna) “od zera” z 5-fold CV i 4 wartościami regularyzacji L2 ===\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1) Budowa klasyfikatora Softmax Regression\n",
    "def make_logreg(X_tr, y_tr, reg, lr=0.1, epochs=200):\n",
    "    n, d = X_tr.shape\n",
    "    classes = np.unique(y_tr)\n",
    "    C = len(classes)\n",
    "    # inicjalizacja wag i biasów\n",
    "    W = np.zeros((d, C))\n",
    "    b = np.zeros(C)\n",
    "    # mapowanie etykiet na indeksy 0..C-1\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    y_idx = np.array([class_to_idx[c] for c in y_tr])\n",
    "    # one-hot\n",
    "    Y = np.eye(C)[y_idx]\n",
    "    # gradient descent\n",
    "    for _ in range(epochs):\n",
    "        scores = X_tr.dot(W) + b                          # (n, C)\n",
    "        exp_s = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
    "        P = exp_s / exp_s.sum(axis=1, keepdims=True)      # (n, C)\n",
    "        # gradient\n",
    "        dW = (X_tr.T.dot(P - Y)) / n + reg * W             # (d, C)\n",
    "        db = (P - Y).mean(axis=0)                          # (C,)\n",
    "        # update\n",
    "        W -= lr * dW\n",
    "        b -= lr * db\n",
    "    # funkcja predykcji\n",
    "    def predict(Xm):\n",
    "        out = Xm.dot(W) + b\n",
    "        return classes[np.argmax(out, axis=1)]\n",
    "    return predict\n",
    "\n",
    "# 2) Funkcja ewaluacji dla danego reg w 5-fold CV\n",
    "def eval_reg(reg):\n",
    "    train_scores, test_scores = [], []\n",
    "    for i in range(len(folds)):\n",
    "        tr_idx = np.hstack([folds[j] for j in range(len(folds)) if j != i])\n",
    "        te_idx = folds[i]\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_te, y_te = X[te_idx], y[te_idx]\n",
    "        clf = make_logreg(X_tr, y_tr, reg)\n",
    "        train_scores.append((clf(X_tr) == y_tr).mean())\n",
    "        test_scores .append((clf(X_te) == y_te).mean())\n",
    "    return np.mean(train_scores), np.mean(test_scores)\n",
    "\n",
    "# 3) Testujemy 4 wartości regularyzacji L2\n",
    "regs = [0.0, 0.01, 0.1, 1.0]\n",
    "results = {r: eval_reg(r) for r in regs}\n",
    "\n",
    "# 4) Wyświetlenie wyników\n",
    "print(\"Softmax Regression – 5-fold CV dla różnych L2 regularization:\")\n",
    "for r, (tr, te) in results.items():\n",
    "    print(f\"  reg = {r:.2f} → train_acc = {tr:.3f}, test_acc = {te:.3f}\")\n",
    "\n",
    "# WNIOSKI\n",
    "# reg = 0.00 → train_acc = 0.207, test_acc = 0.207  \n",
    "#   Brak regularyzacji daje najlepsze wyniki, model nie wykazuje nadmiernego overfittingu.\n",
    "# reg = 0.01 → train_acc = 0.200, test_acc = 0.200  \n",
    "#   Lekka regularyzacja obniża accuracy o ~0.7 p.p., sugerując niewielki wpływ.\n",
    "# reg = 0.10 → train_acc = 0.173, test_acc = 0.172  \n",
    "#   Silniejsza regularyzacja prowadzi do underfittingu i znacznego spadku dokładności.\n",
    "# reg = 1.00 → train_acc = 0.198, test_acc = 0.198  \n",
    "#   Bardzo duże L2 częściowo przywraca dopasowanie, lecz nadal nie dorównuje modelowi bez regularyzacji.\n",
    "\n",
    "# Softmax regression osiąga jedynie około 20 % accuracy niezależnie od poziomu L2, co świadczy o tym, że \n",
    "# liniowe granice decyzyjne są zbyt proste dla tych danych. Brak regularyzacji daje najlepsze rezultaty, co \n",
    "# oznacza, że model nie overfituje znacząco, ale też nie potrafi efektywnie uogólniać. Aby uzyskać sensowną \n",
    "# poprawę wyników, warto sięgnąć po bardziej złożone klasyfikatory lub zaawansowaną inżynierię cech.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43648589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest – 5-fold CV dla różnych n_estimators:\n",
      "  n_estimators =  10 → train_acc = 0.987, test_acc = 0.340\n",
      "  n_estimators =  50 → train_acc = 1.000, test_acc = 0.379\n",
      "  n_estimators = 100 → train_acc = 1.000, test_acc = 0.391\n",
      "  n_estimators = 200 → train_acc = 1.000, test_acc = 0.389\n"
     ]
    }
   ],
   "source": [
    "# === Metoda 5: Random Forest – 5-fold CV dla różnych n_estimators (bez tqdm_notebook) ===\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# 1) Przygotowanie par (train_idx, test_idx) dla 5-fold CV\n",
    "cv_pairs = [\n",
    "    (\n",
    "        np.hstack([folds[j] for j in range(len(folds)) if j != i]),\n",
    "        folds[i]\n",
    "    )\n",
    "    for i in range(len(folds))\n",
    "]\n",
    "\n",
    "# 2) Testujemy różne liczby drzew w lesie\n",
    "n_trees = [10, 50, 100, 200]\n",
    "results = {}\n",
    "for n in n_trees:\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n,\n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    res = cross_validate(\n",
    "        model, X, y,\n",
    "        cv=cv_pairs,\n",
    "        scoring='accuracy',\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    results[n] = (res['train_score'].mean(), res['test_score'].mean())\n",
    "\n",
    "# 3) Wyświetlenie wyników\n",
    "print(\"Random Forest – 5-fold CV dla różnych n_estimators:\")\n",
    "for n, (tr, te) in results.items():\n",
    "    print(f\"  n_estimators = {n:3d} → train_acc = {tr:.3f}, test_acc = {te:.3f}\")\n",
    "\n",
    "# WNIOSKI\n",
    "# n_estimators =  10 → train_acc = 0.987, test_acc = 0.340\n",
    "#   Model jest bliski pełnego dopasowania na treningu, ale generalizacja jest niska.\n",
    "# n_estimators =  50 → train_acc = 1.000, test_acc = 0.379\n",
    "#   Więcej drzew poprawia test_acc o prawie 4 p.p., zmniejszając wariancję.\n",
    "# n_estimators = 100 → train_acc = 1.000, test_acc = 0.391\n",
    "#   Optymalna liczba drzew w badanym zakresie — najlepszy test_acc (~39.1%).\n",
    "# n_estimators = 200 → train_acc = 1.000, test_acc = 0.389\n",
    "#   Dalszy wzrost liczby drzew nie przynosi poprawy, a nawet lekko pogarsza generalizację (efekt malejących korzyści).\n",
    "\n",
    "# Zwiększenie liczby drzew w lesie prowadzi do lepszego uogólnienia – przejście od 10 do 100 drzew \n",
    "# podniosło testową dokładność z 34,0 % do 39,1 %. Jednocześnie nawet przy 10 drzew model osiąga niemal \n",
    "# perfekcyjne dopasowanie na treningu, co wskazuje na dużą wariancję w modelu. Powyżej ~100 drzew korzyści \n",
    "# zaczynają maleć, a dalsze zwiększanie liczby drzew nie poprawia, a wręcz nieznacznie obniża test_acc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
