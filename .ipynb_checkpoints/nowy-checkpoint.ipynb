{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"projekt1_ESI.ipynb - Zoptymalizowana wersja\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/10inHZxNA_b_QBkFcEQaQTieRoGS4ZUZ_\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc  # Garbage collector dla zarządzania pamięcią\n",
    "\n",
    "# Wczytanie danych\n",
    "df = pd.read_csv(\"train_data.csv\", sep=\";\")\n",
    "\n",
    "# Wyświetlenie ogólnych informacji\n",
    "print(\"Kolumny w danych:\\n\", df.columns.tolist(), \"\\n\")\n",
    "print(\"Liczba obserwacji (przed czyszczeniem):\", len(df))\n",
    "\n",
    "# Usunięcie braków danych\n",
    "df = df.dropna()\n",
    "print(\"Liczba obserwacji (po usunięciu braków):\", len(df))\n",
    "\n",
    "# NOWE: Redukcja danych do 80,000 wierszy (losowo)\n",
    "if len(df) > 80000:\n",
    "    df_sampled = df.sample(n=80000, random_state=42)\n",
    "    print(f\"Dane zredukowane losowo do {len(df_sampled)} wierszy\")\n",
    "else:\n",
    "    df_sampled = df.copy()\n",
    "    print(f\"Dane pozostały bez zmian: {len(df_sampled)} wierszy\")\n",
    "\n",
    "# Zwolnienie pamięci\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "# Liczba unikalnych wartości w kolumnie docelowej\n",
    "print(\"\\nUnikalne klasy w kolumnie 'Stay':\")\n",
    "print(df_sampled[\"Stay\"].value_counts())\n",
    "\n",
    "# Automatyczne wygenerowanie opisu tekstowego (do sprawozdania)\n",
    "description = f\"\"\"\n",
    "**Opis problemu klasyfikacyjnego**\n",
    "\n",
    "Celem projektu jest przewidzenie długości pobytu pacjenta w szpitalu na podstawie danych demograficznych i szpitalnych. Problem ma charakter klasyfikacyjny – zmienną docelową jest kolumna **'Stay'**, która określa czas hospitalizacji w kategoriach przedziałów dni (np. '0-10', '11-20', ..., 'More than 100 Days').\n",
    "\n",
    "Dane zawierają {df_sampled.shape[0]} obserwacji i {df_sampled.shape[1]} kolumn. Atrybuty wejściowe obejmują m.in. typ i kod szpitala, kod miasta, dział szpitalny, typ przyjęcia, wiek pacjenta, liczbę odwiedzających oraz kwotę depozytu przy przyjęciu.\n",
    "\n",
    "Zmienna docelowa ('Stay') zawiera następujące klasy (po ich usunięciu z danych modelujących: {['More than 100 Days', '81-90', '91-100', '61-70']}):\n",
    "{df_sampled['Stay'].value_counts().to_string()}\n",
    "\n",
    "Celem modelu jest zaklasyfikowanie nowej obserwacji (pacjenta) do jednej z kategorii długości pobytu. Taki model może wspomagać planowanie zasobów szpitalnych i zarządzanie personelem.\n",
    "\"\"\"\n",
    "\n",
    "print(description)\n",
    "\n",
    "X = df_sampled.drop(\"Stay\", axis=1)\n",
    "y = df_sampled[\"Stay\"]\n",
    "to_remove = [\"More than 100 Days\", \"81-90\", \"91-100\", \"61-70\"]\n",
    "mask = ~y.isin(to_remove)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Zwolnienie pamięci\n",
    "del df_sampled\n",
    "gc.collect()\n",
    "\n",
    "# Liczba próbek na klasę\n",
    "class_counts = y.value_counts().sort_index()\n",
    "print(class_counts)\n",
    "\n",
    "# Wykres rozkładu klas\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "plt.title(\"Rozkład liczby próbek w klasach\")\n",
    "plt.xlabel(\"Klasy\")\n",
    "plt.ylabel(\"Liczba próbek\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# One-hot encoding cech kategorycznych\n",
    "categorical_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols)\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Zwolnienie pamięci\n",
    "del X\n",
    "gc.collect()\n",
    "\n",
    "# One-hot encoding etykiet (kategoryczne)\n",
    "y_cat = pd.Categorical(y)\n",
    "class_names = y_cat.categories\n",
    "y_encoded = pd.get_dummies(y_cat)\n",
    "\n",
    "# ZMODYFIKOWANE: Bardziej konserwatywny oversampling\n",
    "ros = RandomOverSampler(random_state=42, sampling_strategy='auto')\n",
    "X_balanced, y_balanced = ros.fit_resample(X_processed, y_cat)\n",
    "y_balanced_encoded = pd.get_dummies(y_balanced).to_numpy()\n",
    "\n",
    "print(f\"Rozmiar danych po balansowaniu: {X_balanced.shape}\")\n",
    "\n",
    "# Zwolnienie pamięci\n",
    "del X_processed, y_encoded\n",
    "gc.collect()\n",
    "\n",
    "# Skalowanie danych (po oversamplingu)\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_balanced_scaled = scaler.fit_transform(X_balanced)\n",
    "\n",
    "# Zwolnienie pamięci\n",
    "del X_balanced\n",
    "gc.collect()\n",
    "\n",
    "# Podział na zbiór treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_balanced_scaled, y_balanced_encoded, test_size=0.2, random_state=42, stratify=y_balanced\n",
    ")\n",
    "\n",
    "# Zwolnienie pamięci\n",
    "del X_balanced_scaled, y_balanced_encoded\n",
    "gc.collect()\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(y_balanced))\n",
    "\n",
    "# Random Forest - szybkie porównanie\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Zakładam, że y_train i y_test są one-hot encoded\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Trening Random Forest (mniejsza liczba estimatorów dla szybkości)\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train_labels)\n",
    "\n",
    "# Predykcja\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Ewaluacja\n",
    "print(\"Accuracy (Random Forest):\", accuracy_score(y_test_labels, y_pred_rf))\n",
    "print(classification_report(y_test_labels, y_pred_rf))\n",
    "\n",
    "# ZOPTYMALIZOWANA SIEĆ NEURONOWA\n",
    "class OptimizedNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, learning_rate=0.01, lambda_reg=0.0, dropout_rate=0.2):\n",
    "        # Inicjalizacja Xavier/Glorot\n",
    "        self.W1 = np.random.randn(input_size, hidden1_size) * np.sqrt(2. / input_size)\n",
    "        self.b1 = np.zeros((1, hidden1_size))\n",
    "        self.W2 = np.random.randn(hidden1_size, hidden2_size) * np.sqrt(2. / hidden1_size)\n",
    "        self.b2 = np.zeros((1, hidden2_size))\n",
    "        self.W3 = np.random.randn(hidden2_size, output_size) * np.sqrt(2. / hidden2_size)\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Parametry Adam optimizer\n",
    "        self.mW1 = np.zeros_like(self.W1)\n",
    "        self.vW1 = np.zeros_like(self.W1)\n",
    "        self.mb1 = np.zeros_like(self.b1)\n",
    "        self.vb1 = np.zeros_like(self.b1)\n",
    "\n",
    "        self.mW2 = np.zeros_like(self.W2)\n",
    "        self.vW2 = np.zeros_like(self.W2)\n",
    "        self.mb2 = np.zeros_like(self.b2)\n",
    "        self.vb2 = np.zeros_like(self.b2)\n",
    "\n",
    "        self.mW3 = np.zeros_like(self.W3)\n",
    "        self.vW3 = np.zeros_like(self.W3)\n",
    "        self.mb3 = np.zeros_like(self.b3)\n",
    "        self.vb3 = np.zeros_like(self.b3)\n",
    "\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.t = 0\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        # Stabilna wersja softmax\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        eps = 1e-15\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "        # Podstawowa cross-entropy loss\n",
    "        loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "        # Regularizacja L2\n",
    "        if self.lambda_reg > 0:\n",
    "            loss += (self.lambda_reg / 2) * (np.sum(self.W1**2) + np.sum(self.W2**2) + np.sum(self.W3**2))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def dropout(self, A, training=True):\n",
    "        if not training or self.dropout_rate == 0:\n",
    "            return A, np.ones_like(A)\n",
    "\n",
    "        mask = (np.random.rand(*A.shape) > self.dropout_rate).astype(float)\n",
    "        mask /= (1.0 - self.dropout_rate)  # Scaling\n",
    "        return A * mask, mask\n",
    "\n",
    "    def forward(self, X, training=False):\n",
    "        # Warstwa 1\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.a1, self.mask1 = self.dropout(self.a1, training)\n",
    "\n",
    "        # Warstwa 2\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.relu(self.z2)\n",
    "        self.a2, self.mask2 = self.dropout(self.a2, training)\n",
    "\n",
    "        # Warstwa wyjściowa\n",
    "        self.z3 = self.a2 @ self.W3 + self.b3\n",
    "        self.a3 = self.softmax(self.z3)\n",
    "\n",
    "        return self.a3\n",
    "\n",
    "    def adam_update(self, param, grad, m, v):\n",
    "        self.t += 1\n",
    "        m = self.beta1 * m + (1 - self.beta1) * grad\n",
    "        v = self.beta2 * v + (1 - self.beta2) * (grad ** 2)\n",
    "\n",
    "        # Bias correction\n",
    "        m_hat = m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = v / (1 - self.beta2 ** self.t)\n",
    "\n",
    "        # Update\n",
    "        param_update = self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        param -= param_update\n",
    "\n",
    "        return param, m, v\n",
    "\n",
    "    def backward(self, X, y_true):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Backward pass warstwa 3\n",
    "        dz3 = self.a3 - y_true\n",
    "        dW3 = (self.a2.T @ dz3) / m + self.lambda_reg * self.W3\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Backward pass warstwa 2\n",
    "        da2 = dz3 @ self.W3.T\n",
    "        da2 *= self.mask2\n",
    "        dz2 = da2 * self.relu_derivative(self.z2)\n",
    "        dW2 = (self.a1.T @ dz2) / m + self.lambda_reg * self.W2\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Backward pass warstwa 1\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        da1 *= self.mask1\n",
    "        dz1 = da1 * self.relu_derivative(self.z1)\n",
    "        dW1 = (X.T @ dz1) / m + self.lambda_reg * self.W1\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Adam updates\n",
    "        self.W3, self.mW3, self.vW3 = self.adam_update(self.W3, dW3, self.mW3, self.vW3)\n",
    "        self.b3, self.mb3, self.vb3 = self.adam_update(self.b3, db3, self.mb3, self.vb3)\n",
    "\n",
    "        self.W2, self.mW2, self.vW2 = self.adam_update(self.W2, dW2, self.mW2, self.vW2)\n",
    "        self.b2, self.mb2, self.vb2 = self.adam_update(self.b2, db2, self.mb2, self.vb2)\n",
    "\n",
    "        self.W1, self.mW1, self.vW1 = self.adam_update(self.W1, dW1, self.mW1, self.vW1)\n",
    "        self.b1, self.mb1, self.vb1 = self.adam_update(self.b1, db1, self.mb1, self.vb1)\n",
    "\n",
    "    def train_batch(self, X_batch, y_batch, batch_size=1000, epochs=200, X_val=None, y_val=None):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        n_samples = X_batch.shape[0]\n",
    "        n_batches = max(1, n_samples // batch_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "\n",
    "            # Shuffle danych\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_batch[indices]\n",
    "            y_shuffled = y_batch[indices]\n",
    "\n",
    "            # Mini-batch training\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, n_samples)\n",
    "\n",
    "                X_mini = X_shuffled[start_idx:end_idx]\n",
    "                y_mini = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "                # Forward i backward pass\n",
    "                y_pred = self.forward(X_mini, training=True)\n",
    "                loss = self.cross_entropy_loss(y_mini, y_pred)\n",
    "\n",
    "                self.backward(X_mini, y_mini)\n",
    "\n",
    "                epoch_loss += loss\n",
    "                epoch_acc += self.accuracy(y_pred, y_mini)\n",
    "\n",
    "            # Średnie dla epoki\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "            avg_acc = epoch_acc / n_batches\n",
    "\n",
    "            train_losses.append(avg_loss)\n",
    "            train_accuracies.append(avg_acc)\n",
    "\n",
    "            # Walidacja\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_pred = self.forward(X_val, training=False)\n",
    "                val_loss = self.cross_entropy_loss(y_val, val_pred)\n",
    "                val_acc = self.accuracy(val_pred, y_val)\n",
    "\n",
    "                val_losses.append(val_loss)\n",
    "                val_accuracies.append(val_acc)\n",
    "\n",
    "            # Print co 50 epok\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Epoch {epoch}, Train loss: {avg_loss:.4f}, Train acc: {avg_acc:.4f}\", end=\"\")\n",
    "                if X_val is not None:\n",
    "                    print(f\", Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "                else:\n",
    "                    print()\n",
    "\n",
    "        # Wykresy\n",
    "        self.plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies, epochs)\n",
    "\n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def plot_training_history(self, train_losses, val_losses, train_accuracies, val_accuracies, epochs):\n",
    "        epochs_range = np.arange(epochs)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs_range, train_losses, label=\"Train Loss\", alpha=0.8)\n",
    "        if val_losses:\n",
    "            plt.plot(epochs_range, val_losses, label=\"Validation Loss\", alpha=0.8)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss over Epochs\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs_range, train_accuracies, label=\"Train Accuracy\", alpha=0.8)\n",
    "        if val_accuracies:\n",
    "            plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\", alpha=0.8)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Accuracy over Epochs\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X, training=False)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        y_true_labels = np.argmax(y_true, axis=1)\n",
    "        return accuracy_score(y_true_labels, y_pred), y_true_labels, y_pred\n",
    "\n",
    "    def accuracy(self, y_pred, y_true):\n",
    "        pred_labels = np.argmax(y_pred, axis=1)\n",
    "        true_labels = np.argmax(y_true, axis=1)\n",
    "        return np.mean(pred_labels == true_labels)\n",
    "\n",
    "# ZOPTYMALIZOWANE PARAMETRY SIECI\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Output size: {output_size}\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "\n",
    "# Utworzenie sieci z mniejszymi warstwami ukrytymi\n",
    "nn = OptimizedNeuralNetwork(\n",
    "    input_size=input_size,\n",
    "    hidden1_size=128,  # Zmniejszone z 256\n",
    "    hidden2_size=64,   # Zmniejszone z 128\n",
    "    output_size=output_size,\n",
    "    learning_rate=0.001,  # Zwiększone dla szybszej konwergencji\n",
    "    lambda_reg=0.0001,\n",
    "    dropout_rate=0.3  # Zwiększone dropout\n",
    ")\n",
    "\n",
    "# Trening z batch processing\n",
    "print(\"Rozpoczęcie treningu sieci neuronowej...\")\n",
    "train_losses, val_losses = nn.train_batch(\n",
    "    X_train, y_train,\n",
    "    X_val=X_test, y_val=y_test,\n",
    "    batch_size=512,  # Mniejsze batche\n",
    "    epochs=300  # Mniej epok\n",
    ")\n",
    "\n",
    "# Ewaluacja\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EWALUACJA MODELI\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "accuracy, y_true_labels, y_pred = nn.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy (Neural Network): {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report (Neural Network):\")\n",
    "print(classification_report(y_true_labels, y_pred, target_names=class_names))\n",
    "\n",
    "# Macierz pomyłek\n",
    "cm = confusion_matrix(y_true_labels, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Neural Network\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Porównanie z Random Forest\n",
    "print(f\"\\nPorównanie wyników:\")\n",
    "print(f\"Random Forest accuracy: {accuracy_score(y_test_labels, y_pred_rf):.4f}\")\n",
    "print(f\"Neural Network accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Zwolnienie pamięci na końcu\n",
    "gc.collect()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
