{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Wczytywanie i przygotowanie danych\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Wczytuje i przygotowuje dane do analizy\"\"\"\n",
    "    data = pd.read_csv('train_data.csv', sep=';')\n",
    "\n",
    "    # Kodowanie zmiennych kategorycznych\n",
    "    le = LabelEncoder()\n",
    "    categorical_columns = ['Hospital_type_code', 'City_Code_Hospital', 'Hospital_region_code',\n",
    "                          'Department', 'Ward_Type', 'Ward_Facility_Code', 'Type of Admission',\n",
    "                          'Severity of Illness', 'Age','Stay']\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        if col in data.columns:\n",
    "            data[col] = le.fit_transform(data[col].astype(str))\n",
    "\n",
    "    # Usuwanie kolumn identyfikacyjnych\n",
    "    data = data.drop(['case_id', 'patientid'], axis=1, errors='ignore')\n",
    "\n",
    "    # Uzupełnianie brakujących wartości\n",
    "    data = data.fillna(data.mean())\n",
    "\n",
    "    # Konwersja kolumny Stay na zakres numeryczny (problem klasyfikacji)\n",
    "    if 'Stay' in data.columns:\n",
    "        stay_mapping = {'0-10': 0, '11-20': 1, '21-30': 2, '31-40': 3, '41-50': 4,\n",
    "                       '51-60': 5, '61-70': 6, '71-80': 7, '81-90': 8, '91-100': 9, 'More than 100 Days': 10}\n",
    "        data['Stay'] = data['Stay'].map(stay_mapping)\n",
    "        data['Stay'] = data['Stay'].fillna(0)\n",
    "\n",
    "    return data\n",
    "\n",
    "# 1. METODA K-NAJBLIŻSZYCH SĄSIADÓW (KNN)\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean', weight_type='uniform'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.weight_type = weight_type\n",
    "\n",
    "    def calculate_distance(self, x1, x2):\n",
    "        \"\"\"Oblicza odległość między dwoma punktami\"\"\"\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return np.sum(np.abs(x1 - x2))\n",
    "        elif self.distance_metric == 'cosine':\n",
    "            dot_product = np.dot(x1, x2)\n",
    "            norm_x1 = np.linalg.norm(x1)\n",
    "            norm_x2 = np.linalg.norm(x2)\n",
    "            return 1 - (dot_product / (norm_x1 * norm_x2))\n",
    "        else:\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Trenowanie modelu\"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predykcja dla nowych danych\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = []\n",
    "            for i, x_train in enumerate(self.X_train):\n",
    "                dist = self.calculate_distance(x, x_train)\n",
    "                distances.append((dist, self.y_train[i]))\n",
    "\n",
    "            # Sortowanie po odległości i wybór k najbliższych\n",
    "            distances.sort(key=lambda x: x[0])\n",
    "            k_nearest = distances[:self.k]\n",
    "\n",
    "            if self.weight_type == 'uniform':\n",
    "                # Głosowanie większością\n",
    "                votes = [vote for _, vote in k_nearest]\n",
    "                prediction = Counter(votes).most_common(1)[0][0]\n",
    "            else:\n",
    "                # Ważone głosowanie\n",
    "                weighted_votes = {}\n",
    "                for dist, vote in k_nearest:\n",
    "                    weight = 1 / (dist + 1e-8)  # Dodajemy małą wartość aby uniknąć dzielenia przez 0\n",
    "                    if vote in weighted_votes:\n",
    "                        weighted_votes[vote] += weight\n",
    "                    else:\n",
    "                        weighted_votes[vote] = weight\n",
    "                prediction = max(weighted_votes, key=weighted_votes.get)\n",
    "\n",
    "            predictions.append(prediction)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# 2. DRZEWO DECYZYJNE\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_leaf=1, criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "    def calculate_impurity(self, y):\n",
    "        \"\"\"Oblicza nieczystość węzła\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "\n",
    "        if self.criterion == 'gini':\n",
    "            impurity = 1 - sum((count/total)**2 for count in counts.values())\n",
    "        else:  # entropy\n",
    "            impurity = -sum((count/total) * np.log2(count/total + 1e-8) for count in counts.values())\n",
    "\n",
    "        return impurity\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        \"\"\"Znajduje najlepszy podział\"\"\"\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_gain = -1\n",
    "\n",
    "        current_impurity = self.calculate_impurity(y)\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if np.sum(left_mask) < self.min_samples_leaf or np.sum(right_mask) < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                left_y = y[left_mask]\n",
    "                right_y = y[right_mask]\n",
    "\n",
    "                left_impurity = self.calculate_impurity(left_y)\n",
    "                right_impurity = self.calculate_impurity(right_y)\n",
    "\n",
    "                gain = current_impurity - (len(left_y)/len(y) * left_impurity +\n",
    "                                         len(right_y)/len(y) * right_impurity)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold, best_gain\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        \"\"\"Buduje drzewo rekurencyjnie\"\"\"\n",
    "        if (depth >= self.max_depth or\n",
    "            len(np.unique(y)) == 1 or\n",
    "            len(y) < 2 * self.min_samples_leaf):\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "        feature, threshold, gain = self.find_best_split(X, y)\n",
    "\n",
    "        if feature is None or gain <= 0:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        node = {\n",
    "            'feature': feature,\n",
    "            'threshold': threshold,\n",
    "            'left': self.build_tree(X[left_mask], y[left_mask], depth + 1),\n",
    "            'right': self.build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        }\n",
    "\n",
    "        return node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Trenowanie drzewa\"\"\"\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def predict_single(self, x, tree):\n",
    "        \"\"\"Predykcja dla pojedynczej próbki\"\"\"\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "\n",
    "        if x[tree['feature']] <= tree['threshold']:\n",
    "            return self.predict_single(x, tree['left'])\n",
    "        else:\n",
    "            return self.predict_single(x, tree['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predykcja dla zbioru danych\"\"\"\n",
    "        return np.array([self.predict_single(x, self.tree) for x in X])\n",
    "\n",
    "# 3. NAIWNY KLASYFIKATOR BAYESA\n",
    "class NaiveBayes:\n",
    "    def __init__(self, smoothing=1.0, distribution='gaussian'):\n",
    "        self.smoothing = smoothing\n",
    "        self.distribution = distribution\n",
    "        self.class_priors = {}\n",
    "        self.feature_stats = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Trenowanie klasyfikatora\"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples = len(y)\n",
    "\n",
    "        for cls in self.classes:\n",
    "            class_mask = (y == cls)\n",
    "            self.class_priors[cls] = np.sum(class_mask) / n_samples\n",
    "\n",
    "            X_class = X[class_mask]\n",
    "\n",
    "            if self.distribution == 'gaussian':\n",
    "                # Parametry rozkładu normalnego\n",
    "                self.feature_stats[cls] = {\n",
    "                    'mean': np.mean(X_class, axis=0),\n",
    "                    'std': np.std(X_class, axis=0) + 1e-8  # Dodajemy małą wartość dla stabilności\n",
    "                }\n",
    "            else:\n",
    "                # Dyskretny rozkład (multinomial)\n",
    "                self.feature_stats[cls] = {}\n",
    "                for feature in range(X.shape[1]):\n",
    "                    feature_values = X_class[:, feature]\n",
    "                    value_counts = Counter(feature_values)\n",
    "                    total_count = len(feature_values)\n",
    "\n",
    "                    # Wygładzanie Laplace'a\n",
    "                    self.feature_stats[cls][feature] = {}\n",
    "                    for value in np.unique(X[:, feature]):\n",
    "                        count = value_counts.get(value, 0)\n",
    "                        probability = (count + self.smoothing) / (total_count + self.smoothing * len(np.unique(X[:, feature])))\n",
    "                        self.feature_stats[cls][feature][value] = probability\n",
    "\n",
    "    def calculate_likelihood(self, x, cls):\n",
    "        \"\"\"Oblicza prawdopodobieństwo cechy dla danej klasy\"\"\"\n",
    "        if self.distribution == 'gaussian':\n",
    "            mean = self.feature_stats[cls]['mean']\n",
    "            std = self.feature_stats[cls]['std']\n",
    "\n",
    "            # Prawdopodobieństwo z rozkładu normalnego\n",
    "            likelihood = np.prod(1 / (np.sqrt(2 * np.pi) * std) *\n",
    "                               np.exp(-0.5 * ((x - mean) / std) ** 2))\n",
    "        else:\n",
    "            likelihood = 1.0\n",
    "            for feature in range(len(x)):\n",
    "                value = x[feature]\n",
    "                if value in self.feature_stats[cls][feature]:\n",
    "                    likelihood *= self.feature_stats[cls][feature][value]\n",
    "                else:\n",
    "                    # Nieznana wartość - używamy wygładzania\n",
    "                    unique_values = len(self.feature_stats[cls][feature])\n",
    "                    likelihood *= self.smoothing / (sum(self.feature_stats[cls][feature].values()) * unique_values + self.smoothing)\n",
    "\n",
    "        return likelihood\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predykcja dla zbioru danych\"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        for x in X:\n",
    "            class_probabilities = {}\n",
    "\n",
    "            for cls in self.classes:\n",
    "                prior = self.class_priors[cls]\n",
    "                likelihood = self.calculate_likelihood(x, cls)\n",
    "                class_probabilities[cls] = prior * likelihood\n",
    "\n",
    "            predicted_class = max(class_probabilities, key=class_probabilities.get)\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "# FUNKCJE POMOCNICZE\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    \"\"\"Oblicza dokładność\"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def test_knn_parameters(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Testuje różne parametry dla KNN\"\"\"\n",
    "    print(\"=== ANALIZA PARAMETRÓW KNN ===\")\n",
    "\n",
    "    # Test różnych wartości k\n",
    "    k_values = [3, 5, 7, 9]\n",
    "    k_results = []\n",
    "\n",
    "    for k in k_values:\n",
    "        knn = KNN(k=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        k_results.append(accuracy)\n",
    "        print(f\"k={k}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    # Test różnych metryk odległości\n",
    "    distance_metrics = ['euclidean', 'manhattan', 'cosine']\n",
    "    distance_results = []\n",
    "\n",
    "    print(\"\\nTest metryk odległości:\")\n",
    "    for metric in distance_metrics:\n",
    "        knn = KNN(k=5, distance_metric=metric)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        distance_results.append(accuracy)\n",
    "        print(f\"{metric}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    # Test różnych typów wag\n",
    "    weight_types = ['uniform', 'distance']\n",
    "    weight_results = []\n",
    "\n",
    "    print(\"\\nTest typów wag:\")\n",
    "    for weight in weight_types:\n",
    "        knn = KNN(k=5, weight_type=weight)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        weight_results.append(accuracy)\n",
    "        print(f\"{weight}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    return k_results, distance_results, weight_results\n",
    "\n",
    "def test_tree_parameters(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Testuje różne parametry dla Drzewa Decyzyjnego\"\"\"\n",
    "    print(\"\\n=== ANALIZA PARAMETRÓW DRZEWA DECYZYJNEGO ===\")\n",
    "\n",
    "    # Test różnych głębokości\n",
    "    max_depths = [3, 5, 7, 10]\n",
    "    depth_results = []\n",
    "\n",
    "    for depth in max_depths:\n",
    "        tree = DecisionTree(max_depth=depth)\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        depth_results.append(accuracy)\n",
    "        print(f\"max_depth={depth}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    # Test różnych min_samples_leaf\n",
    "    min_samples = [1, 3, 5, 10]\n",
    "    samples_results = []\n",
    "\n",
    "    print(\"\\nTest min_samples_leaf:\")\n",
    "    for min_sample in min_samples:\n",
    "        tree = DecisionTree(max_depth=5, min_samples_leaf=min_sample)\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        samples_results.append(accuracy)\n",
    "        print(f\"min_samples_leaf={min_sample}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    # Test różnych kryteriów\n",
    "    criteria = ['gini', 'entropy']\n",
    "    criteria_results = []\n",
    "\n",
    "    print(\"\\nTest kryteriów podziału:\")\n",
    "    for criterion in criteria:\n",
    "        tree = DecisionTree(max_depth=5, criterion=criterion)\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        criteria_results.append(accuracy)\n",
    "        print(f\"{criterion}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    return depth_results, samples_results, criteria_results\n",
    "\n",
    "def test_nb_parameters(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Testuje różne parametry dla Naiwnego Bayesa\"\"\"\n",
    "    print(\"\\n=== ANALIZA PARAMETRÓW NAIWNEGO BAYESA ===\")\n",
    "\n",
    "    # Test różnych wartości wygładzania\n",
    "    smoothing_values = [0.1, 0.5, 1.0, 2.0]\n",
    "    smoothing_results = []\n",
    "\n",
    "    for smoothing in smoothing_values:\n",
    "        nb = NaiveBayes(smoothing=smoothing)\n",
    "        nb.fit(X_train, y_train)\n",
    "        y_pred = nb.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        smoothing_results.append(accuracy)\n",
    "        print(f\"smoothing={smoothing}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    # Test różnych rozkładów\n",
    "    distributions = ['gaussian', 'multinomial']\n",
    "    distribution_results = []\n",
    "\n",
    "    print(\"\\nTest rozkładów:\")\n",
    "    for dist in distributions:\n",
    "        nb = NaiveBayes(distribution=dist)\n",
    "        nb.fit(X_train, y_train)\n",
    "        y_pred = nb.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        distribution_results.append(accuracy)\n",
    "        print(f\"{dist}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    return smoothing_results, distribution_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Główna funkcja programu\"\"\"\n",
    "    print(\"PROJEKT UCZENIA MASZYNOWEGO\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Wczytanie danych\n",
    "    data = load_and_prepare_data()\n",
    "    print(f\"Wczytano dane: {data.shape}\")\n",
    "    print(f\"Kolumny: {list(data.columns)}\")\n",
    "\n",
    "    # Przygotowanie danych do trenowania\n",
    "    if 'Stay' in data.columns:\n",
    "        X = data.drop('Stay', axis=1).values\n",
    "        y = data['Stay'].values\n",
    "    else:\n",
    "        print(\"Nie znaleziono kolumny docelowej 'Stay'\")\n",
    "        return\n",
    "\n",
    "    # Podział na zbiory treningowy i testowy\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    print(f\"Zbiór treningowy: {X_train.shape}\")\n",
    "    print(f\"Zbiór testowy: {X_test.shape}\")\n",
    "    print(f\"Klasy: {np.unique(y)}\")\n",
    "\n",
    "    # Testowanie parametrów dla każdej metody\n",
    "    knn_results = test_knn_parameters(X_train, X_test, y_train, y_test)\n",
    "    tree_results = test_tree_parameters(X_train, X_test, y_train, y_test)\n",
    "    nb_results = test_nb_parameters(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Podsumowanie\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"PODSUMOWANIE I WNIOSKI:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"\\n1. K-NAJBLIŻSZYCH SĄSIADÓW (KNN):\")\n",
    "    print(\"   - Metoda oparta na podobieństwie między próbkami\")\n",
    "    print(\"   - Parametr k wpływa na kompromis między overfitting a underfitting\")\n",
    "    print(\"   - Różne metryki odległości mogą dawać różne wyniki\")\n",
    "\n",
    "    print(\"\\n2. DRZEWO DECYZYJNE:\")\n",
    "    print(\"   - Metoda tworząca reguły decyzyjne w formie drzewa\")\n",
    "    print(\"   - Głębokość drzewa kontroluje złożoność modelu\")\n",
    "    print(\"   - min_samples_leaf zapobiega przeuczeniu\")\n",
    "\n",
    "    print(\"\\n3. NAIWNY KLASYFIKATOR BAYESA:\")\n",
    "    print(\"   - Metoda probabilistyczna oparta na twierdzeniu Bayesa\")\n",
    "    print(\"   - Wygładzanie pomaga w przypadku rzadkich wartości\")\n",
    "    print(\"   - Zakłada niezależność cech (naiwne założenie)\")\n",
    "\n",
    "    print(\"\\nOGÓLNE WNIOSKI:\")\n",
    "    print(\"- Wybór parametrów znacząco wpływa na skuteczność modeli\")\n",
    "    print(\"- Każda metoda ma swoje zalety i wady\")\n",
    "    print(\"- Ważne jest testowanie różnych konfiguracji parametrów\")\n",
    "    print(\"- Wyniki mogą się różnić w zależności od charakterystyki danych\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
