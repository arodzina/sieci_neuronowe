{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-27T19:47:30.500966Z",
     "start_time": "2025-05-27T19:11:39.035402Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Wczytywanie i przygotowanie danych\n",
    "def diagnose_data(filepath='train_data.csv'):\n",
    "    \"\"\"Diagnozuje strukturę danych w pliku CSV\"\"\"\n",
    "    try:\n",
    "        # Próbuj różne separatory\n",
    "        separators = [';', ',', '\\t', '|']\n",
    "        for sep in separators:\n",
    "            try:\n",
    "                data = pd.read_csv(filepath, sep=sep, nrows=5)  # Wczytaj tylko 5 wierszy do testu\n",
    "                if data.shape[1] > 1:  # Jeśli więcej niż jedna kolumna, separator prawdopodobnie poprawny\n",
    "                    print(f\"Znaleziono separator: '{sep}'\")\n",
    "                    print(f\"Kształt danych: {data.shape}\")\n",
    "                    print(f\"Kolumny: {list(data.columns)}\")\n",
    "                    print(f\"Pierwsze 3 wiersze:\")\n",
    "                    print(data.head(3))\n",
    "                    return sep\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(\"Nie udało się automatycznie wykryć separatora\")\n",
    "        return ';'  # domyślny\n",
    "    except Exception as e:\n",
    "        print(f\"Błąd podczas wczytywania pliku: {e}\")\n",
    "        return ';'\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Wczytuje i przygotowuje dane do analizy\"\"\"\n",
    "    # Najpierw zdiagnozuj dane\n",
    "    print(\"=== DIAGNOZA DANYCH ===\")\n",
    "    separator = diagnose_data()\n",
    "\n",
    "    # Wczytaj pełne dane z wykrytym separatorem\n",
    "    data = pd.read_csv('train_data.csv', sep=separator)\n",
    "\n",
    "    # Kodowanie zmiennych kategorycznych\n",
    "    le = LabelEncoder()\n",
    "    categorical_columns = ['Hospital_type_code', 'City_Code_Hospital', 'Hospital_region_code',\n",
    "                          'Department', 'Ward_Type', 'Ward_Facility_Code', 'Type of Admission',\n",
    "                          'Severity of Illness', 'Age','Stay']\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        if col in data.columns:\n",
    "            data[col] = le.fit_transform(data[col].astype(str))\n",
    "\n",
    "    # Usuwanie kolumn identyfikacyjnych\n",
    "    data = data.drop(['case_id', 'patientid'], axis=1, errors='ignore')\n",
    "\n",
    "    # Uzupełnianie brakujących wartości\n",
    "    data = data.fillna(data.mean())\n",
    "\n",
    "    # Konwersja kolumny Stay na zakres numeryczny (problem klasyfikacji)\n",
    "    # Sprawdzenie różnych możliwych nazw kolumny docelowej\n",
    "    target_column = None\n",
    "    possible_targets = ['Stay', 'stay', 'target', 'Target', 'class', 'Class', 'label', 'Label']\n",
    "\n",
    "    for col in possible_targets:\n",
    "        if col in data.columns:\n",
    "            target_column = col\n",
    "            break\n",
    "\n",
    "    if target_column is not None and target_column in data.columns:\n",
    "        # Sprawdzenie czy kolumna zawiera stringi wymagające mapowania\n",
    "        if data[target_column].dtype == 'object':\n",
    "            stay_mapping = {'0-10': 0, '11-20': 1, '21-30': 2, '31-40': 3, '41-50': 4,\n",
    "                           '51-60': 5, '61-70': 6, '71-80': 7, '81-90': 8, '91-100': 9, 'More than 100 Days': 10}\n",
    "            data[target_column] = data[target_column].map(stay_mapping)\n",
    "            data[target_column] = data[target_column].fillna(0)\n",
    "\n",
    "    return data\n",
    "\n",
    "# 1. METODA K-NAJBLIŻSZYCH SĄSIADÓW (KNN)\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean', weight_type='uniform'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.weight_type = weight_type\n",
    "\n",
    "    def calculate_distance(self, x1, x2):\n",
    "        \"\"\"Oblicza odległość między dwoma punktami\"\"\"\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return np.sum(np.abs(x1 - x2))\n",
    "        elif self.distance_metric == 'cosine':\n",
    "            dot_product = np.dot(x1, x2)\n",
    "            norm_x1 = np.linalg.norm(x1)\n",
    "            norm_x2 = np.linalg.norm(x2)\n",
    "            return 1 - (dot_product / (norm_x1 * norm_x2))\n",
    "        else:\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Trenowanie modelu\"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predykcja dla nowych danych\"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = []\n",
    "            for i, x_train in enumerate(self.X_train):\n",
    "                dist = self.calculate_distance(x, x_train)\n",
    "                distances.append((dist, self.y_train[i]))\n",
    "\n",
    "            # Sortowanie po odległości i wybór k najbliższych\n",
    "            distances.sort(key=lambda x: x[0])\n",
    "            k_nearest = distances[:self.k]\n",
    "\n",
    "            if self.weight_type == 'uniform':\n",
    "                # Głosowanie większością\n",
    "                votes = [vote for _, vote in k_nearest]\n",
    "                prediction = Counter(votes).most_common(1)[0][0]\n",
    "            else:\n",
    "                # Ważone głosowanie\n",
    "                weighted_votes = {}\n",
    "                for dist, vote in k_nearest:\n",
    "                    weight = 1 / (dist + 1e-8)  # Dodajemy małą wartość aby uniknąć dzielenia przez 0\n",
    "                    if vote in weighted_votes:\n",
    "                        weighted_votes[vote] += weight\n",
    "                    else:\n",
    "                        weighted_votes[vote] = weight\n",
    "                prediction = max(weighted_votes, key=weighted_votes.get)\n",
    "\n",
    "            predictions.append(prediction)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# 2. DRZEWO DECYZYJNE\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_leaf=1, criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "    def calculate_impurity(self, y):\n",
    "        \"\"\"Oblicza nieczystość węzła\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "\n",
    "        if self.criterion == 'gini':\n",
    "            impurity = 1 - sum((count/total)**2 for count in counts.values())\n",
    "        else:  # entropy\n",
    "            impurity = -sum((count/total) * np.log2(count/total + 1e-8) for count in counts.values())\n",
    "\n",
    "        return impurity\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        \"\"\"Znajduje najlepszy podział\"\"\"\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_gain = -1\n",
    "\n",
    "        current_impurity = self.calculate_impurity(y)\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if np.sum(left_mask) < self.min_samples_leaf or np.sum(right_mask) < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                left_y = y[left_mask]\n",
    "                right_y = y[right_mask]\n",
    "\n",
    "                left_impurity = self.calculate_impurity(left_y)\n",
    "                right_impurity = self.calculate_impurity(right_y)\n",
    "\n",
    "                gain = current_impurity - (len(left_y)/len(y) * left_impurity +\n",
    "                                         len(right_y)/len(y) * right_impurity)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold, best_gain\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        \"\"\"Buduje drzewo rekurencyjnie\"\"\"\n",
    "        if (depth >= self.max_depth or\n",
    "            len(np.unique(y)) == 1 or\n",
    "            len(y) < 2 * self.min_samples_leaf):\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "        feature, threshold, gain = self.find_best_split(X, y)\n",
    "\n",
    "        if feature is None or gain <= 0:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        node = {\n",
    "            'feature': feature,\n",
    "            'threshold': threshold,\n",
    "            'left': self.build_tree(X[left_mask], y[left_mask], depth + 1),\n",
    "            'right': self.build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        }\n",
    "\n",
    "        return node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Trenowanie drzewa\"\"\"\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def predict_single(self, x, tree):\n",
    "        \"\"\"Predykcja dla pojedynczej próbki\"\"\"\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "\n",
    "        if x[tree['feature']] <= tree['threshold']:\n",
    "            return self.predict_single(x, tree['left'])\n",
    "        else:\n",
    "            return self.predict_single(x, tree['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predykcja dla zbioru danych\"\"\"\n",
    "        return np.array([self.predict_single(x, self.tree) for x in X])\n",
    "\n",
    "# 3. NAIWNY KLASYFIKATOR BAYESA\n",
    "class NaiveBayes:\n",
    "    def __init__(self, smoothing=1.0, distribution='gaussian'):\n",
    "        self.smoothing = smoothing\n",
    "        self.distribution = distribution\n",
    "        self.class_priors = {}\n",
    "        self.feature_stats = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Trenowanie klasyfikatora\"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples = len(y)\n",
    "\n",
    "        for cls in self.classes:\n",
    "            class_mask = (y == cls)\n",
    "            self.class_priors[cls] = np.sum(class_mask) / n_samples\n",
    "\n",
    "            X_class = X[class_mask]\n",
    "\n",
    "            if self.distribution == 'gaussian':\n",
    "                # Parametry rozkładu normalnego\n",
    "                self.feature_stats[cls] = {\n",
    "                    'mean': np.mean(X_class, axis=0),\n",
    "                    'std': np.std(X_class, axis=0) + 1e-8  # Dodajemy małą wartość dla stabilności\n",
    "                }\n",
    "            else:\n",
    "                # Dyskretny rozkład (multinomial)\n",
    "                self.feature_stats[cls] = {}\n",
    "                for feature in range(X.shape[1]):\n",
    "                    feature_values = X_class[:, feature]\n",
    "                    value_counts = Counter(feature_values)\n",
    "                    total_count = len(feature_values)\n",
    "\n",
    "                    # Wygładzanie Laplace'a\n",
    "                    self.feature_stats[cls][feature] = {}\n",
    "                    for value in np.unique(X[:, feature]):\n",
    "                        count = value_counts.get(value, 0)\n",
    "                        probability = (count + self.smoothing) / (total_count + self.smoothing * len(np.unique(X[:, feature])))\n",
    "                        self.feature_stats[cls][feature][value] = probability\n",
    "\n",
    "    def calculate_likelihood(self, x, cls):\n",
    "        \"\"\"Oblicza prawdopodobieństwo cechy dla danej klasy\"\"\"\n",
    "        if self.distribution == 'gaussian':\n",
    "            mean = self.feature_stats[cls]['mean']\n",
    "            std = self.feature_stats[cls]['std']\n",
    "\n",
    "            # Prawdopodobieństwo z rozkładu normalnego\n",
    "            likelihood = np.prod(1 / (np.sqrt(2 * np.pi) * std) *\n",
    "                               np.exp(-0.5 * ((x - mean) / std) ** 2))\n",
    "        else:\n",
    "            likelihood = 1.0\n",
    "            for feature in range(len(x)):\n",
    "                value = x[feature]\n",
    "                if value in self.feature_stats[cls][feature]:\n",
    "                    likelihood *= self.feature_stats[cls][feature][value]\n",
    "                else:\n",
    "                    # Nieznana wartość - używamy wygładzania\n",
    "                    unique_values = len(self.feature_stats[cls][feature])\n",
    "                    likelihood *= self.smoothing / (sum(self.feature_stats[cls][feature].values()) * unique_values + self.smoothing)\n",
    "\n",
    "        return likelihood\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predykcja dla zbioru danych\"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        for x in X:\n",
    "            class_probabilities = {}\n",
    "\n",
    "            for cls in self.classes:\n",
    "                prior = self.class_priors[cls]\n",
    "                likelihood = self.calculate_likelihood(x, cls)\n",
    "                class_probabilities[cls] = prior * likelihood\n",
    "\n",
    "            predicted_class = max(class_probabilities, key=class_probabilities.get)\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "# FUNKCJE POMOCNICZE\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    \"\"\"Oblicza dokładność\"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def test_knn_parameters(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Testuje różne parametry dla KNN\"\"\"\n",
    "    print(\"=== ANALIZA PARAMETRÓW KNN ===\")\n",
    "\n",
    "    # Test różnych wartości k\n",
    "    k_values = [3, 5, 7, 9]\n",
    "    k_results = []\n",
    "\n",
    "    for k in k_values:\n",
    "        knn = KNN(k=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        k_results.append(accuracy)\n",
    "        print(f\"k={k}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    # Test różnych metryk odległości\n",
    "    distance_metrics = ['euclidean', 'manhattan', 'cosine']\n",
    "    distance_results = []\n",
    "\n",
    "    print(\"\\nTest metryk odległości:\")\n",
    "    for metric in distance_metrics:\n",
    "        knn = KNN(k=5, distance_metric=metric)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        distance_results.append(accuracy)\n",
    "        print(f\"{metric}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    # Test różnych typów wag\n",
    "    weight_types = ['uniform', 'distance']\n",
    "    weight_results = []\n",
    "\n",
    "    print(\"\\nTest typów wag:\")\n",
    "    for weight in weight_types:\n",
    "        knn = KNN(k=5, weight_type=weight)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        weight_results.append(accuracy)\n",
    "        print(f\"{weight}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    return k_results, distance_results, weight_results\n",
    "\n",
    "def test_tree_parameters(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Testuje różne parametry dla Drzewa Decyzyjnego\"\"\"\n",
    "    print(\"\\n=== ANALIZA PARAMETRÓW DRZEWA DECYZYJNEGO ===\")\n",
    "\n",
    "    # Test różnych głębokości\n",
    "    max_depths = [3, 5, 7, 10]\n",
    "    depth_results = []\n",
    "\n",
    "    for depth in max_depths:\n",
    "        tree = DecisionTree(max_depth=depth)\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        depth_results.append(accuracy)\n",
    "        print(f\"max_depth={depth}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    # Test różnych min_samples_leaf\n",
    "    min_samples = [1, 3, 5, 10]\n",
    "    samples_results = []\n",
    "\n",
    "    print(\"\\nTest min_samples_leaf:\")\n",
    "    for min_sample in min_samples:\n",
    "        tree = DecisionTree(max_depth=5, min_samples_leaf=min_sample)\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        samples_results.append(accuracy)\n",
    "        print(f\"min_samples_leaf={min_sample}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    # Test różnych kryteriów\n",
    "    criteria = ['gini', 'entropy']\n",
    "    criteria_results = []\n",
    "\n",
    "    print(\"\\nTest kryteriów podziału:\")\n",
    "    for criterion in criteria:\n",
    "        tree = DecisionTree(max_depth=5, criterion=criterion)\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        criteria_results.append(accuracy)\n",
    "        print(f\"{criterion}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    return depth_results, samples_results, criteria_results\n",
    "\n",
    "def test_nb_parameters(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Testuje różne parametry dla Naiwnego Bayesa\"\"\"\n",
    "    print(\"\\n=== ANALIZA PARAMETRÓW NAIWNEGO BAYESA ===\")\n",
    "\n",
    "    # Test różnych wartości wygładzania\n",
    "    smoothing_values = [0.1, 0.5, 1.0, 2.0]\n",
    "    smoothing_results = []\n",
    "\n",
    "    for smoothing in smoothing_values:\n",
    "        nb = NaiveBayes(smoothing=smoothing)\n",
    "        nb.fit(X_train, y_train)\n",
    "        y_pred = nb.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        smoothing_results.append(accuracy)\n",
    "        print(f\"smoothing={smoothing}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    # Test różnych rozkładów\n",
    "    distributions = ['gaussian', 'multinomial']\n",
    "    distribution_results = []\n",
    "\n",
    "    print(\"\\nTest rozkładów:\")\n",
    "    for dist in distributions:\n",
    "        nb = NaiveBayes(distribution=dist)\n",
    "        nb.fit(X_train, y_train)\n",
    "        y_pred = nb.predict(X_test)\n",
    "        accuracy = calculate_accuracy(y_test, y_pred)\n",
    "        distribution_results.append(accuracy)\n",
    "        print(f\"{dist}: Dokładność = {accuracy:.4f}\")\n",
    "\n",
    "    return smoothing_results, distribution_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Główna funkcja programu\"\"\"\n",
    "    print(\"PROJEKT UCZENIA MASZYNOWEGO\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Wczytanie danych\n",
    "    data = load_and_prepare_data()\n",
    "    print(f\"Wczytano pełne dane: {data.shape}\")\n",
    "\n",
    "    # LOSOWY WYBÓR 10% DANYCH\n",
    "    np.random.seed(42)  # Dla powtarzalności wyników\n",
    "    sample_size = int(0.05 * len(data))  # 10% danych\n",
    "    random_indices = np.random.choice(len(data), size=sample_size, replace=False)\n",
    "    data_sample = data.iloc[random_indices].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Wybrano losową próbkę 10% danych: {data_sample.shape}\")\n",
    "    print(f\"Kolumny: {list(data_sample.columns)}\")\n",
    "\n",
    "    # Przygotowanie danych do trenowania (używamy próbki)\n",
    "    # Sprawdzenie dostępnych kolumn i automatyczne znalezienie kolumny docelowej\n",
    "    target_column = None\n",
    "    possible_targets = ['Stay', 'stay', 'target', 'Target', 'class', 'Class', 'label', 'Label']\n",
    "\n",
    "    for col in possible_targets:\n",
    "        if col in data_sample.columns:\n",
    "            target_column = col\n",
    "            break\n",
    "\n",
    "    if target_column is None:\n",
    "        print(\"Dostępne kolumny:\", list(data_sample.columns))\n",
    "        print(\"Nie znaleziono standardowej kolumny docelowej. Proszę wybrać kolumnę docelową z listy powyżej.\")\n",
    "        # Przyjmijmy ostatnią kolumnę jako docelową\n",
    "        target_column = data_sample.columns[-1]\n",
    "        print(f\"Używam ostatniej kolumny jako docelowej: '{target_column}'\")\n",
    "\n",
    "    X = data_sample.drop(target_column, axis=1).values\n",
    "    y = data_sample[target_column].values\n",
    "\n",
    "    print(f\"Kolumna docelowa: '{target_column}'\")\n",
    "\n",
    "    # Podział na zbiory treningowy i testowy\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    print(f\"Zbiór treningowy: {X_train.shape}\")\n",
    "    print(f\"Zbiór testowy: {X_test.shape}\")\n",
    "    print(f\"Klasy: {np.unique(y)}\")\n",
    "\n",
    "    # Sprawdzenie czy mamy wystarczającą ilość danych dla każdej klasy\n",
    "    print(f\"Rozkład klas w próbce:\")\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"  Klasa {cls}: {count} próbek\")\n",
    "\n",
    "    # Testowanie parametrów dla każdej metody\n",
    "    knn_results = test_knn_parameters(X_train, X_test, y_train, y_test)\n",
    "    tree_results = test_tree_parameters(X_train, X_test, y_train, y_test)\n",
    "    nb_results = test_nb_parameters(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Podsumowanie\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"PODSUMOWANIE I WNIOSKI:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"UWAGA: Analiza przeprowadzona na losowej próbce 10% danych ({sample_size} próbek)\")\n",
    "\n",
    "    print(\"\\n1. K-NAJBLIŻSZYCH SĄSIADÓW (KNN):\")\n",
    "    print(\"   - Metoda oparta na podobieństwie między próbkami\")\n",
    "    print(\"   - Parametr k wpływa na kompromis między overfitting a underfitting\")\n",
    "    print(\"   - Różne metryki odległości mogą dawać różne wyniki\")\n",
    "\n",
    "    print(\"\\n2. DRZEWO DECYZYJNE:\")\n",
    "    print(\"   - Metoda tworząca reguły decyzyjne w formie drzewa\")\n",
    "    print(\"   - Głębokość drzewa kontroluje złożoność modelu\")\n",
    "    print(\"   - min_samples_leaf zapobiega przeuczeniu\")\n",
    "\n",
    "    print(\"\\n3. NAIWNY KLASYFIKATOR BAYESA:\")\n",
    "    print(\"   - Metoda probabilistyczna oparta na twierdzeniu Bayesa\")\n",
    "    print(\"   - Wygładzanie pomaga w przypadku rzadkich wartości\")\n",
    "    print(\"   - Zakłada niezależność cech (naiwne założenie)\")\n",
    "\n",
    "    print(\"\\nOGÓLNE WNIOSKI:\")\n",
    "    print(\"- Wybór parametrów znacząco wpływa na skuteczność modeli\")\n",
    "    print(\"- Każda metoda ma swoje zalety i wady\")\n",
    "    print(\"- Ważne jest testowanie różnych konfiguracji parametrów\")\n",
    "    print(\"- Wyniki mogą się różnić w zależności od charakterystyki danych\")\n",
    "    print(\"- Analiza na próbce 10% danych może nie odzwierciedlać pełnej charakterystyki zbioru\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJEKT UCZENIA MASZYNOWEGO\n",
      "==================================================\n",
      "=== DIAGNOZA DANYCH ===\n",
      "Znaleziono separator: ';'\n",
      "Kształt danych: (5, 18)\n",
      "Kolumny: ['case_id', 'Hospital_code', 'Hospital_type_code', 'City_Code_Hospital', 'Hospital_region_code', 'Available Extra Rooms in Hospital', 'Department', 'Ward_Type', 'Ward_Facility_Code', 'Bed Grade', 'patientid', 'City_Code_Patient', 'Type of Admission', 'Severity of Illness', 'Visitors with Patient', 'Age', 'Admission_Deposit', 'Stay']\n",
      "Pierwsze 3 wiersze:\n",
      "   case_id  Hospital_code Hospital_type_code  City_Code_Hospital  \\\n",
      "0        1              8                  c                   3   \n",
      "1        2              2                  c                   5   \n",
      "2        3             10                  e                   1   \n",
      "\n",
      "  Hospital_region_code  Available Extra Rooms in Hospital    Department  \\\n",
      "0                    Z                                  3  radiotherapy   \n",
      "1                    Z                                  2  radiotherapy   \n",
      "2                    X                                  2    anesthesia   \n",
      "\n",
      "  Ward_Type Ward_Facility_Code  Bed Grade  patientid  City_Code_Patient  \\\n",
      "0         R                  F        2.0      31397                7.0   \n",
      "1         S                  F        2.0      31397                7.0   \n",
      "2         S                  E        2.0      31397                7.0   \n",
      "\n",
      "  Type of Admission Severity of Illness  Visitors with Patient    Age  \\\n",
      "0         Emergency             Extreme                      2  51-60   \n",
      "1            Trauma             Extreme                      2  51-60   \n",
      "2            Trauma             Extreme                      2  51-60   \n",
      "\n",
      "   Admission_Deposit   Stay  \n",
      "0             4911.0   0-10  \n",
      "1             5954.0  41-50  \n",
      "2             4745.0  31-40  \n",
      "Wczytano pełne dane: (318438, 16)\n",
      "Wybrano losową próbkę 10% danych: (15921, 16)\n",
      "Kolumny: ['Hospital_code', 'Hospital_type_code', 'City_Code_Hospital', 'Hospital_region_code', 'Available Extra Rooms in Hospital', 'Department', 'Ward_Type', 'Ward_Facility_Code', 'Bed Grade', 'City_Code_Patient', 'Type of Admission', 'Severity of Illness', 'Visitors with Patient', 'Age', 'Admission_Deposit', 'Stay']\n",
      "Kolumna docelowa: 'Stay'\n",
      "Zbiór treningowy: (11144, 15)\n",
      "Zbiór testowy: (4777, 15)\n",
      "Klasy: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "Rozkład klas w próbce:\n",
      "  Klasa 0: 1186 próbek\n",
      "  Klasa 1: 3901 próbek\n",
      "  Klasa 2: 4345 próbek\n",
      "  Klasa 3: 2792 próbek\n",
      "  Klasa 4: 551 próbek\n",
      "  Klasa 5: 1795 próbek\n",
      "  Klasa 6: 131 próbek\n",
      "  Klasa 7: 518 próbek\n",
      "  Klasa 8: 228 próbek\n",
      "  Klasa 9: 141 próbek\n",
      "  Klasa 10: 333 próbek\n",
      "=== ANALIZA PARAMETRÓW KNN ===\n",
      "k=3: Dokładność = 0.2148\n",
      "k=5: Dokładność = 0.2263\n",
      "k=7: Dokładność = 0.2451\n",
      "k=9: Dokładność = 0.2470\n",
      "\n",
      "Test metryk odległości:\n",
      "euclidean: Dokładność = 0.2263\n",
      "manhattan: Dokładność = 0.2499\n",
      "cosine: Dokładność = 0.2954\n",
      "\n",
      "Test typów wag:\n",
      "uniform: Dokładność = 0.2263\n",
      "distance: Dokładność = 0.2284\n",
      "\n",
      "=== ANALIZA PARAMETRÓW DRZEWA DECYZYJNEGO ===\n",
      "max_depth=3: Dokładność = 0.3592\n",
      "max_depth=5: Dokładność = 0.3896\n",
      "max_depth=7: Dokładność = 0.3860\n",
      "max_depth=10: Dokładność = 0.3716\n",
      "\n",
      "Test min_samples_leaf:\n",
      "min_samples_leaf=1: Dokładność = 0.3896\n",
      "min_samples_leaf=3: Dokładność = 0.3896\n",
      "min_samples_leaf=5: Dokładność = 0.3896\n",
      "min_samples_leaf=10: Dokładność = 0.3906\n",
      "\n",
      "Test kryteriów podziału:\n",
      "gini: Dokładność = 0.3896\n",
      "entropy: Dokładność = 0.3825\n",
      "\n",
      "=== ANALIZA PARAMETRÓW NAIWNEGO BAYESA ===\n",
      "smoothing=0.1: Dokładność = 0.3668\n",
      "smoothing=0.5: Dokładność = 0.3668\n",
      "smoothing=1.0: Dokładność = 0.3668\n",
      "smoothing=2.0: Dokładność = 0.3668\n",
      "\n",
      "Test rozkładów:\n",
      "gaussian: Dokładność = 0.3668\n",
      "multinomial: Dokładność = 0.3220\n",
      "\n",
      "==================================================\n",
      "PODSUMOWANIE I WNIOSKI:\n",
      "==================================================\n",
      "UWAGA: Analiza przeprowadzona na losowej próbce 10% danych (15921 próbek)\n",
      "\n",
      "1. K-NAJBLIŻSZYCH SĄSIADÓW (KNN):\n",
      "   - Metoda oparta na podobieństwie między próbkami\n",
      "   - Parametr k wpływa na kompromis między overfitting a underfitting\n",
      "   - Różne metryki odległości mogą dawać różne wyniki\n",
      "\n",
      "2. DRZEWO DECYZYJNE:\n",
      "   - Metoda tworząca reguły decyzyjne w formie drzewa\n",
      "   - Głębokość drzewa kontroluje złożoność modelu\n",
      "   - min_samples_leaf zapobiega przeuczeniu\n",
      "\n",
      "3. NAIWNY KLASYFIKATOR BAYESA:\n",
      "   - Metoda probabilistyczna oparta na twierdzeniu Bayesa\n",
      "   - Wygładzanie pomaga w przypadku rzadkich wartości\n",
      "   - Zakłada niezależność cech (naiwne założenie)\n",
      "\n",
      "OGÓLNE WNIOSKI:\n",
      "- Wybór parametrów znacząco wpływa na skuteczność modeli\n",
      "- Każda metoda ma swoje zalety i wady\n",
      "- Ważne jest testowanie różnych konfiguracji parametrów\n",
      "- Wyniki mogą się różnić w zależności od charakterystyki danych\n",
      "- Analiza na próbce 10% danych może nie odzwierciedlać pełnej charakterystyki zbioru\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T19:47:30.528951Z",
     "start_time": "2025-05-27T19:47:30.526952Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "88a59f24b3cb353e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
